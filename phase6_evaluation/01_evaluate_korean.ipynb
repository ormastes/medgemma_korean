{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Phase 5.1: Evaluate Korean Medical Capabilities\n",
    "\n",
    "Evaluate the model on Korean medical benchmarks.\n",
    "\n",
    "## Contents\n",
    "1. Setup\n",
    "2. Load Model\n",
    "3. Evaluate on KorMedMCQA\n",
    "4. Qualitative Evaluation\n",
    "5. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "\n",
    "# GPU setup\n",
    "from config.gpu_utils import setup_gpu, print_memory_usage\n",
    "device = setup_gpu()\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Directories\n# Primary: Use instruction-tuned model\nMODEL_DIR = \"../models/instruction_tuned\"\n\n# Alternative: Use expanded model directly (before instruction tuning)\n# MODEL_DIR = \"../models/final/korean_medgemma_expanded\"\n\n# Legacy (non-expanded):\n# MODEL_DIR = \"../models/final/korean_medgemma\"\n\nEVAL_DATA_DIR = \"../data/processed/kormedmcqa_eval\"\nRESULTS_DIR = \"../results\"\n\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\nprint(f\"Model: {MODEL_DIR}\")\nprint(f\"Results: {RESULTS_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "\n",
    "# For inference, load in 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KorMedMCQA test set\n",
    "if os.path.exists(EVAL_DATA_DIR):\n",
    "    eval_dataset = load_from_disk(EVAL_DATA_DIR)\n",
    "    print(f\"Loaded evaluation dataset: {len(eval_dataset)} examples\")\n",
    "else:\n",
    "    # Load directly from HuggingFace\n",
    "    print(\"Loading KorMedMCQA from HuggingFace...\")\n",
    "    eval_dataset = load_dataset(\"sean0042/KorMedMCQA\", split=\"test\")\n",
    "    print(f\"Loaded: {len(eval_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview sample\n",
    "sample = eval_dataset[0]\n",
    "print(\"Sample evaluation example:\")\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Choices: A={sample['A']}, B={sample['B']}, C={sample['C']}, D={sample['D']}, E={sample['E']}\")\n",
    "print(f\"Answer: {sample['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Evaluate on KorMedMCQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mcqa_prompt(example):\n",
    "    \"\"\"Create evaluation prompt for MCQA\"\"\"\n",
    "    \n",
    "    question = example[\"question\"]\n",
    "    choices = []\n",
    "    for letter in ['A', 'B', 'C', 'D', 'E']:\n",
    "        if letter in example and example[letter]:\n",
    "            choices.append(f\"{letter}. {example[letter]}\")\n",
    "    \n",
    "    formatted_choices = \"\\n\".join(choices)\n",
    "    \n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "당신은 한국어 의료 전문 AI 어시스턴트입니다. 정확하고 도움이 되는 의료 정보를 제공하세요.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "다음 의료 관련 질문에 답하세요. 정답 알파벳(A, B, C, D, E 중 하나)만 답하세요.\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "선택지:\n",
    "{formatted_choices}\n",
    "\n",
    "정답:\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(response):\n",
    "    \"\"\"Extract answer letter from model response\"\"\"\n",
    "    \n",
    "    # Look for A, B, C, D, E at the beginning\n",
    "    response = response.strip().upper()\n",
    "    \n",
    "    # Check for direct letter\n",
    "    for letter in ['A', 'B', 'C', 'D', 'E']:\n",
    "        if response.startswith(letter):\n",
    "            return letter\n",
    "    \n",
    "    # Check for number mapping\n",
    "    number_to_letter = {'1': 'A', '2': 'B', '3': 'C', '4': 'D', '5': 'E'}\n",
    "    for num, letter in number_to_letter.items():\n",
    "        if num in response[:5]:\n",
    "            return letter\n",
    "    \n",
    "    # Check anywhere in response\n",
    "    for letter in ['A', 'B', 'C', 'D', 'E']:\n",
    "        if letter in response[:20]:\n",
    "            return letter\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert answer index to letter\n",
    "def answer_idx_to_letter(idx):\n",
    "    \"\"\"Convert 1-indexed answer to letter\"\"\"\n",
    "    mapping = {1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E'}\n",
    "    return mapping.get(idx, 'A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "print(\"\\nRunning KorMedMCQA evaluation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "results = []\n",
    "\n",
    "# Limit for testing (remove for full evaluation)\n",
    "max_samples = min(len(eval_dataset), 100)  # Evaluate first 100 for speed\n",
    "\n",
    "for i, example in enumerate(tqdm(eval_dataset, total=max_samples)):\n",
    "    if i >= max_samples:\n",
    "        break\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = create_mcqa_prompt(example)\n",
    "    \n",
    "    # Generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            temperature=0.1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode response (only new tokens)\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # Extract predicted answer\n",
    "    predicted = extract_answer(response)\n",
    "    \n",
    "    # Get correct answer\n",
    "    correct_answer = answer_idx_to_letter(example[\"answer\"])\n",
    "    \n",
    "    # Check correctness\n",
    "    is_correct = predicted == correct_answer\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    \n",
    "    # Save result\n",
    "    results.append({\n",
    "        \"question\": example[\"question\"],\n",
    "        \"predicted\": predicted,\n",
    "        \"correct_answer\": correct_answer,\n",
    "        \"is_correct\": is_correct,\n",
    "        \"response\": response,\n",
    "    })\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"KorMedMCQA Results\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples\n",
    "print(\"\\nSample predictions:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(min(5, len(results))):\n",
    "    r = results[i]\n",
    "    status = \"✓\" if r[\"is_correct\"] else \"✗\"\n",
    "    print(f\"\\n{status} Q: {r['question'][:80]}...\")\n",
    "    print(f\"   Predicted: {r['predicted']}, Correct: {r['correct_answer']}\")\n",
    "    print(f\"   Response: {r['response'][:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Qualitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with open-ended questions\n",
    "test_questions = [\n",
    "    \"고혈압의 주요 증상과 위험 요인은 무엇인가요?\",\n",
    "    \"당뇨병 환자가 일상에서 주의해야 할 점은 무엇인가요?\",\n",
    "    \"감기와 독감의 차이점을 설명해주세요.\",\n",
    "    \"두통이 자주 발생할 때 어떻게 대처해야 하나요?\",\n",
    "]\n",
    "\n",
    "print(\"Qualitative evaluation (open-ended questions):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for question in test_questions:\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "당신은 한국어 의료 전문 AI 어시스턴트입니다.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {response[:500]}...\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "eval_results = {\n",
    "    \"model\": MODEL_DIR,\n",
    "    \"benchmark\": \"KorMedMCQA\",\n",
    "    \"accuracy\": accuracy,\n",
    "    \"correct\": correct,\n",
    "    \"total\": total,\n",
    "    \"results\": results,\n",
    "}\n",
    "\n",
    "results_path = f\"{RESULTS_DIR}/kormedmcqa_eval.json\"\n",
    "with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(eval_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Korean Medical Evaluation Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nKorMedMCQA Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"\\nResults saved to: {results_path}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  Run 02_evaluate_english.ipynb to check English retention\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}