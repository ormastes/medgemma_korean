{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1.3: Merge Korean Tokens with MedGemma Tokenizer\n",
    "\n",
    "Merge filtered Korean tokens into the MedGemma tokenizer.\n",
    "\n",
    "## Contents\n",
    "1. Load MedGemma Tokenizer\n",
    "2. Load Filtered Korean Tokens\n",
    "3. Filter Duplicates\n",
    "4. Add New Tokens\n",
    "5. Verify Merged Tokenizer\n",
    "6. Save Merged Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directories\n",
    "MODEL_DIR = \"../models/tokenizer\"\n",
    "MERGED_DIR = \"../models/merged_tokenizer\"\n",
    "\n",
    "os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Merged tokenizer directory: {MERGED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load MedGemma/Gemma Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base tokenizer\n",
    "# Note: Use actual MedGemma when available, or Gemma base for testing\n",
    "BASE_MODEL = \"google/gemma-2b\"  # Change to \"google/medgemma-4b-it\" when available\n",
    "\n",
    "print(f\"Loading base tokenizer: {BASE_MODEL}\")\n",
    "\n",
    "try:\n",
    "    base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "    print(f\"Loaded successfully!\")\n",
    "    print(f\"Original vocabulary size: {len(base_tokenizer)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    print(\"Make sure you have access to the model (may require login)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze base tokenizer\n",
    "print(\"\\nBase tokenizer info:\")\n",
    "print(f\"  Vocab size: {len(base_tokenizer)}\")\n",
    "print(f\"  Model max length: {base_tokenizer.model_max_length}\")\n",
    "print(f\"  Padding side: {base_tokenizer.padding_side}\")\n",
    "\n",
    "print(\"\\nSpecial tokens:\")\n",
    "for name, token in base_tokenizer.special_tokens_map.items():\n",
    "    print(f\"  {name}: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Korean tokenization with base tokenizer\n",
    "test_korean = \"안녕하세요, 저는 의료 AI 어시스턴트입니다.\"\n",
    "\n",
    "base_tokens = base_tokenizer.tokenize(test_korean)\n",
    "base_ids = base_tokenizer.encode(test_korean)\n",
    "\n",
    "print(f\"Korean text: {test_korean}\")\n",
    "print(f\"Base tokenizer tokens ({len(base_tokens)}): {base_tokens}\")\n",
    "print(f\"\\nNote: Each Korean character may be split into multiple byte tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Filtered Korean Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filtered Korean tokens\n",
    "filtered_tokens_path = f\"{MODEL_DIR}/filtered_korean_tokens.txt\"\n",
    "\n",
    "with open(filtered_tokens_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    korean_tokens = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Loaded {len(korean_tokens)} filtered Korean tokens\")\n",
    "print(f\"\\nSample tokens: {korean_tokens[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Filter Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get existing vocabulary\n",
    "base_vocab = set(base_tokenizer.get_vocab().keys())\n",
    "print(f\"Base vocabulary size: {len(base_vocab)}\")\n",
    "\n",
    "# Filter out tokens already in base vocabulary\n",
    "new_tokens = []\n",
    "duplicate_tokens = []\n",
    "\n",
    "for token in korean_tokens:\n",
    "    # Clean SentencePiece prefix for comparison\n",
    "    # Note: Different tokenizers may use different prefixes\n",
    "    clean_token = token.replace(\"▁\", \"\")\n",
    "    \n",
    "    # Check various forms\n",
    "    if token in base_vocab or clean_token in base_vocab:\n",
    "        duplicate_tokens.append(token)\n",
    "    else:\n",
    "        new_tokens.append(clean_token)  # Use clean version for HF tokenizer\n",
    "\n",
    "print(f\"\\nDuplicate tokens (already in base): {len(duplicate_tokens)}\")\n",
    "print(f\"New tokens to add: {len(new_tokens)}\")\n",
    "\n",
    "if duplicate_tokens:\n",
    "    print(f\"\\nSample duplicates: {duplicate_tokens[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty and whitespace-only tokens\n",
    "new_tokens = [t for t in new_tokens if t and not t.isspace()]\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "seen = set()\n",
    "unique_new_tokens = []\n",
    "for token in new_tokens:\n",
    "    if token not in seen:\n",
    "        seen.add(token)\n",
    "        unique_new_tokens.append(token)\n",
    "\n",
    "new_tokens = unique_new_tokens\n",
    "print(f\"Unique new tokens: {len(new_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Add New Tokens to Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record original vocab size\n",
    "original_vocab_size = len(base_tokenizer)\n",
    "print(f\"Original vocabulary size: {original_vocab_size}\")\n",
    "\n",
    "# Add new tokens\n",
    "print(f\"\\nAdding {len(new_tokens)} new tokens...\")\n",
    "num_added = base_tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "print(f\"Tokens added: {num_added}\")\n",
    "print(f\"New vocabulary size: {len(base_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify some new tokens were added\n",
    "new_vocab_size = len(base_tokenizer)\n",
    "\n",
    "print(f\"\\nVocabulary expansion:\")\n",
    "print(f\"  Before: {original_vocab_size}\")\n",
    "print(f\"  After: {new_vocab_size}\")\n",
    "print(f\"  Added: {new_vocab_size - original_vocab_size}\")\n",
    "\n",
    "# Check specific tokens\n",
    "test_tokens = [\"의사\", \"환자\", \"병원\", \"치료\", \"진단\"]\n",
    "print(f\"\\nTest token IDs:\")\n",
    "for token in test_tokens:\n",
    "    token_id = base_tokenizer.convert_tokens_to_ids(token)\n",
    "    if token_id != base_tokenizer.unk_token_id:\n",
    "        print(f\"  {token}: {token_id} (added)\")\n",
    "    else:\n",
    "        print(f\"  {token}: UNK (not found as single token)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Verify Merged Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Korean tokenization with merged tokenizer\n",
    "test_sentences = [\n",
    "    \"안녕하세요, 저는 의료 AI 어시스턴트입니다.\",\n",
    "    \"환자가 발열과 기침 증상을 호소합니다.\",\n",
    "    \"당뇨병은 혈당 조절에 문제가 생기는 대사 질환입니다.\",\n",
    "    \"MRI 검사 결과 뇌에 이상 소견이 발견되었습니다.\",\n",
    "]\n",
    "\n",
    "print(\"Tokenization comparison (before vs after):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Reload original tokenizer for comparison\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    original_tokens = original_tokenizer.tokenize(sentence)\n",
    "    merged_tokens = base_tokenizer.tokenize(sentence)\n",
    "    \n",
    "    improvement = len(original_tokens) / len(merged_tokens) if merged_tokens else 0\n",
    "    \n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    print(f\"Original ({len(original_tokens)} tokens): {original_tokens[:15]}...\")\n",
    "    print(f\"Merged ({len(merged_tokens)} tokens): {merged_tokens[:15]}...\")\n",
    "    print(f\"Improvement: {improvement:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify English still works correctly\n",
    "english_test = \"The patient presents with symptoms of diabetes mellitus.\"\n",
    "\n",
    "original_en_tokens = original_tokenizer.tokenize(english_test)\n",
    "merged_en_tokens = base_tokenizer.tokenize(english_test)\n",
    "\n",
    "print(\"English tokenization check:\")\n",
    "print(f\"\\nSentence: {english_test}\")\n",
    "print(f\"Original ({len(original_en_tokens)} tokens): {original_en_tokens}\")\n",
    "print(f\"Merged ({len(merged_en_tokens)} tokens): {merged_en_tokens}\")\n",
    "print(f\"\\nEnglish tokenization preserved: {original_en_tokens == merged_en_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encode/decode roundtrip\n",
    "test_text = \"환자의 혈압이 140/90 mmHg로 고혈압 소견입니다.\"\n",
    "\n",
    "encoded = base_tokenizer.encode(test_text)\n",
    "decoded = base_tokenizer.decode(encoded)\n",
    "\n",
    "print(\"Encode/Decode roundtrip test:\")\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "print(f\"Match: {test_text == decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Save Merged Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged tokenizer\n",
    "base_tokenizer.save_pretrained(MERGED_DIR)\n",
    "print(f\"Saved merged tokenizer to {MERGED_DIR}\")\n",
    "\n",
    "# List saved files\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(MERGED_DIR):\n",
    "    size = os.path.getsize(os.path.join(MERGED_DIR, f))\n",
    "    print(f\"  {f}: {size/1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save token mapping for embedding initialization\n",
    "token_mapping = {\n",
    "    \"base_model\": BASE_MODEL,\n",
    "    \"original_vocab_size\": original_vocab_size,\n",
    "    \"new_vocab_size\": new_vocab_size,\n",
    "    \"new_tokens_count\": new_vocab_size - original_vocab_size,\n",
    "    \"new_tokens\": new_tokens,\n",
    "    \"new_token_ids\": {\n",
    "        token: base_tokenizer.convert_tokens_to_ids(token)\n",
    "        for token in new_tokens\n",
    "        if base_tokenizer.convert_tokens_to_ids(token) != base_tokenizer.unk_token_id\n",
    "    },\n",
    "}\n",
    "\n",
    "mapping_path = f\"{MERGED_DIR}/token_mapping.json\"\n",
    "with open(mapping_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(token_mapping, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved token mapping to {mapping_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Tokenizer Merge Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBase model: {BASE_MODEL}\")\n",
    "print(f\"Original vocab size: {original_vocab_size}\")\n",
    "print(f\"New vocab size: {new_vocab_size}\")\n",
    "print(f\"Korean tokens added: {new_vocab_size - original_vocab_size}\")\n",
    "print(f\"\\nMerged tokenizer saved to: {MERGED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Phase 1: Tokenizer Preparation Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Move to Phase 2: Embedding Initialization\")\n",
    "print(\"  2. Run phase2_embedding/01_resize_embeddings.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
