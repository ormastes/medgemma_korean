{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1.1: Train Korean SentencePiece Tokenizer\n",
    "\n",
    "Train a SentencePiece tokenizer on Korean corpus for vocabulary expansion.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Load Corpus\n",
    "2. Train SentencePiece Tokenizer\n",
    "3. Analyze Token Distribution\n",
    "4. Test Tokenization\n",
    "5. Save Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import sentencepiece as spm\n",
    "from collections import Counter\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"../data\"\n",
    "RAW_DIR = f\"{DATA_DIR}/raw\"\n",
    "MODEL_DIR = \"../models/tokenizer\"\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Model directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for corpus file\n",
    "corpus_path = f\"{RAW_DIR}/korean_corpus_for_tokenizer.txt\"\n",
    "\n",
    "if os.path.exists(corpus_path):\n",
    "    file_size_gb = os.path.getsize(corpus_path) / (1024**3)\n",
    "    print(f\"Corpus file: {corpus_path}\")\n",
    "    print(f\"Size: {file_size_gb:.2f} GB\")\n",
    "    \n",
    "    # Count lines\n",
    "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        line_count = sum(1 for _ in f)\n",
    "    print(f\"Lines: {line_count:,}\")\n",
    "else:\n",
    "    print(f\"Corpus file not found: {corpus_path}\")\n",
    "    print(\"Run phase0_data_preparation/02_collect_korean_medical.ipynb first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview corpus\n",
    "print(\"Corpus preview:\")\n",
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        print(f\"\\nLine {i+1}: {line[:200]}...\")\n",
    "        if i >= 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Train SentencePiece Tokenizer\n",
    "\n",
    "Following EEVE methodology:\n",
    "- Train intermediate tokenizer with 40,000 tokens\n",
    "- Will filter by frequency in next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePiece training parameters (EEVE-style)\n",
    "VOCAB_SIZE = 40000  # Intermediate size, will filter later\n",
    "MODEL_PREFIX = f\"{MODEL_DIR}/korean_sp\"\n",
    "\n",
    "training_params = {\n",
    "    \"input\": corpus_path,\n",
    "    \"model_prefix\": MODEL_PREFIX,\n",
    "    \"vocab_size\": VOCAB_SIZE,\n",
    "    \"character_coverage\": 0.9995,  # High coverage for Korean\n",
    "    \"model_type\": \"bpe\",  # BPE like most LLMs\n",
    "    \"pad_id\": 0,\n",
    "    \"unk_id\": 1,\n",
    "    \"bos_id\": 2,\n",
    "    \"eos_id\": 3,\n",
    "    \"num_threads\": 16,\n",
    "    \"train_extremely_large_corpus\": True,\n",
    "    \"max_sentence_length\": 16384,\n",
    "    \"input_sentence_size\": 5000000,  # Sample 5M sentences for training\n",
    "    \"shuffle_input_sentence\": True,\n",
    "}\n",
    "\n",
    "print(\"Training parameters:\")\n",
    "for key, value in training_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tokenizer\n",
    "print(\"\\nTraining SentencePiece tokenizer...\")\n",
    "print(\"This may take 10-30 minutes depending on corpus size.\")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**training_params)\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Model saved to: {MODEL_PREFIX}.model\")\n",
    "print(f\"Vocab saved to: {MODEL_PREFIX}.vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify output files\n",
    "model_file = f\"{MODEL_PREFIX}.model\"\n",
    "vocab_file = f\"{MODEL_PREFIX}.vocab\"\n",
    "\n",
    "if os.path.exists(model_file):\n",
    "    model_size_mb = os.path.getsize(model_file) / (1024**2)\n",
    "    print(f\"Model file: {model_file} ({model_size_mb:.2f} MB)\")\n",
    "\n",
    "if os.path.exists(vocab_file):\n",
    "    vocab_size_mb = os.path.getsize(vocab_file) / (1024**2)\n",
    "    print(f\"Vocab file: {vocab_file} ({vocab_size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load and Analyze Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(f\"{MODEL_PREFIX}.model\")\n",
    "\n",
    "print(f\"Vocabulary size: {sp.GetPieceSize()}\")\n",
    "print(f\"\\nSpecial tokens:\")\n",
    "print(f\"  PAD: {sp.pad_id()} -> '{sp.IdToPiece(sp.pad_id())}'\")\n",
    "print(f\"  UNK: {sp.unk_id()} -> '{sp.IdToPiece(sp.unk_id())}'\")\n",
    "print(f\"  BOS: {sp.bos_id()} -> '{sp.IdToPiece(sp.bos_id())}'\")\n",
    "print(f\"  EOS: {sp.eos_id()} -> '{sp.IdToPiece(sp.eos_id())}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vocabulary composition\n",
    "def analyze_vocabulary(sp_processor):\n",
    "    \"\"\"Analyze token types in vocabulary\"\"\"\n",
    "    \n",
    "    korean_tokens = []\n",
    "    english_tokens = []\n",
    "    number_tokens = []\n",
    "    special_tokens = []\n",
    "    other_tokens = []\n",
    "    \n",
    "    for i in range(sp_processor.GetPieceSize()):\n",
    "        piece = sp_processor.IdToPiece(i)\n",
    "        \n",
    "        # Remove SentencePiece prefix\n",
    "        clean_piece = piece.replace(\"▁\", \"\")\n",
    "        \n",
    "        if not clean_piece:\n",
    "            special_tokens.append(piece)\n",
    "        elif any('가' <= c <= '힣' for c in clean_piece):\n",
    "            korean_tokens.append(piece)\n",
    "        elif clean_piece.isascii() and clean_piece.isalpha():\n",
    "            english_tokens.append(piece)\n",
    "        elif clean_piece.isdigit():\n",
    "            number_tokens.append(piece)\n",
    "        else:\n",
    "            other_tokens.append(piece)\n",
    "    \n",
    "    return {\n",
    "        \"korean\": korean_tokens,\n",
    "        \"english\": english_tokens,\n",
    "        \"numbers\": number_tokens,\n",
    "        \"special\": special_tokens,\n",
    "        \"other\": other_tokens,\n",
    "    }\n",
    "\n",
    "vocab_analysis = analyze_vocabulary(sp)\n",
    "\n",
    "print(\"Vocabulary composition:\")\n",
    "for category, tokens in vocab_analysis.items():\n",
    "    pct = len(tokens) / sp.GetPieceSize() * 100\n",
    "    print(f\"  {category}: {len(tokens)} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample tokens from each category\n",
    "print(\"\\nSample Korean tokens:\")\n",
    "print(vocab_analysis[\"korean\"][:30])\n",
    "\n",
    "print(\"\\nSample English tokens:\")\n",
    "print(vocab_analysis[\"english\"][:20])\n",
    "\n",
    "print(\"\\nSample other tokens:\")\n",
    "print(vocab_analysis[\"other\"][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize vocabulary distribution\n",
    "categories = list(vocab_analysis.keys())\n",
    "counts = [len(tokens) for tokens in vocab_analysis.values()]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(categories, counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
    "plt.title('Korean Tokenizer Vocabulary Composition')\n",
    "plt.xlabel('Token Category')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "for i, (cat, count) in enumerate(zip(categories, counts)):\n",
    "    plt.text(i, count + 200, f'{count}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{MODEL_DIR}/vocab_composition.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Chart saved to {MODEL_DIR}/vocab_composition.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Test Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"안녕하세요, 저는 의료 AI 어시스턴트입니다.\",\n",
    "    \"환자가 발열과 기침 증상을 호소합니다.\",\n",
    "    \"당뇨병은 혈당 조절에 문제가 생기는 대사 질환입니다.\",\n",
    "    \"MRI 검사 결과 뇌에 이상 소견이 발견되었습니다.\",\n",
    "    \"고혈압 환자는 염분 섭취를 줄여야 합니다.\",\n",
    "]\n",
    "\n",
    "print(\"Tokenization test:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    pieces = sp.EncodeAsPieces(sentence)\n",
    "    ids = sp.EncodeAsIds(sentence)\n",
    "    \n",
    "    print(f\"\\nOriginal: {sentence}\")\n",
    "    print(f\"Tokens ({len(pieces)}): {pieces}\")\n",
    "    print(f\"IDs: {ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with original MedGemma tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"Comparing with MedGemma tokenizer...\")\n",
    "\n",
    "try:\n",
    "    medgemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")  # Use base Gemma for comparison\n",
    "    \n",
    "    print(\"\\nToken count comparison:\")\n",
    "    print(f\"{'Sentence':<50} | {'Korean SP':>10} | {'Gemma':>10} | {'Ratio':>8}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        korean_tokens = len(sp.EncodeAsPieces(sentence))\n",
    "        gemma_tokens = len(medgemma_tokenizer.encode(sentence))\n",
    "        ratio = gemma_tokens / korean_tokens\n",
    "        \n",
    "        short_sentence = sentence[:47] + \"...\" if len(sentence) > 50 else sentence\n",
    "        print(f\"{short_sentence:<50} | {korean_tokens:>10} | {gemma_tokens:>10} | {ratio:>7.2f}x\")\n",
    "    \n",
    "    print(\"\\nNote: Lower ratio with Korean tokenizer = more efficient Korean encoding\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load Gemma tokenizer: {e}\")\n",
    "    print(\"This comparison is optional.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Save Tokenizer Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer info\n",
    "tokenizer_info = {\n",
    "    \"model_path\": f\"{MODEL_PREFIX}.model\",\n",
    "    \"vocab_path\": f\"{MODEL_PREFIX}.vocab\",\n",
    "    \"vocab_size\": sp.GetPieceSize(),\n",
    "    \"model_type\": \"bpe\",\n",
    "    \"character_coverage\": 0.9995,\n",
    "    \"special_tokens\": {\n",
    "        \"pad_id\": sp.pad_id(),\n",
    "        \"unk_id\": sp.unk_id(),\n",
    "        \"bos_id\": sp.bos_id(),\n",
    "        \"eos_id\": sp.eos_id(),\n",
    "    },\n",
    "    \"vocabulary_composition\": {\n",
    "        category: len(tokens) for category, tokens in vocab_analysis.items()\n",
    "    },\n",
    "}\n",
    "\n",
    "info_path = f\"{MODEL_DIR}/tokenizer_info.json\"\n",
    "with open(info_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tokenizer_info, f, indent=2)\n",
    "\n",
    "print(f\"Tokenizer info saved to {info_path}\")\n",
    "print(json.dumps(tokenizer_info, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Korean tokens list for filtering\n",
    "korean_tokens_path = f\"{MODEL_DIR}/korean_tokens_all.txt\"\n",
    "with open(korean_tokens_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for token in vocab_analysis[\"korean\"]:\n",
    "        f.write(f\"{token}\\n\")\n",
    "\n",
    "print(f\"Korean tokens saved to {korean_tokens_path}\")\n",
    "print(f\"Total Korean tokens: {len(vocab_analysis['korean'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Korean Tokenizer Training Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTokenizer saved to: {MODEL_DIR}\")\n",
    "print(f\"Vocabulary size: {sp.GetPieceSize()}\")\n",
    "print(f\"Korean tokens: {len(vocab_analysis['korean'])}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Run 02_filter_tokens.ipynb to filter by frequency\")\n",
    "print(\"  2. Run 03_merge_tokenizers.ipynb to merge with MedGemma\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
