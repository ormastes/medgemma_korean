{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1.2: Filter Tokens by Frequency\n",
    "\n",
    "Filter Korean tokens by frequency (EEVE method: keep tokens >= 6000 occurrences).\n",
    "\n",
    "## Contents\n",
    "1. Load Tokenizer and Corpus\n",
    "2. Count Token Frequencies\n",
    "3. Filter by Threshold\n",
    "4. Analyze Filtered Tokens\n",
    "5. Save Filtered Token List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import sentencepiece as spm\n",
    "from collections import Counter\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"../data\"\n",
    "RAW_DIR = f\"{DATA_DIR}/raw\"\n",
    "MODEL_DIR = \"../models/tokenizer\"\n",
    "\n",
    "print(f\"Model directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Tokenizer and Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained tokenizer\n",
    "model_path = f\"{MODEL_DIR}/korean_sp.model\"\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(model_path)\n",
    "\n",
    "print(f\"Loaded tokenizer: {model_path}\")\n",
    "print(f\"Vocabulary size: {sp.GetPieceSize()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus path\n",
    "corpus_path = f\"{RAW_DIR}/korean_corpus_for_tokenizer.txt\"\n",
    "\n",
    "if os.path.exists(corpus_path):\n",
    "    file_size_gb = os.path.getsize(corpus_path) / (1024**3)\n",
    "    print(f\"Corpus: {corpus_path}\")\n",
    "    print(f\"Size: {file_size_gb:.2f} GB\")\n",
    "else:\n",
    "    print(f\"Corpus not found: {corpus_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Count Token Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count token frequencies in corpus\n",
    "print(\"Counting token frequencies...\")\n",
    "print(\"This may take several minutes for large corpora.\")\n",
    "\n",
    "token_counts = Counter()\n",
    "total_tokens = 0\n",
    "lines_processed = 0\n",
    "\n",
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"Processing corpus\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = sp.EncodeAsPieces(line)\n",
    "        token_counts.update(tokens)\n",
    "        total_tokens += len(tokens)\n",
    "        lines_processed += 1\n",
    "        \n",
    "        # Progress update\n",
    "        if lines_processed % 500000 == 0:\n",
    "            print(f\"  Processed {lines_processed:,} lines, {total_tokens:,} tokens\")\n",
    "\n",
    "print(f\"\\nTotal lines processed: {lines_processed:,}\")\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "print(f\"Unique tokens: {len(token_counts):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show most common tokens\n",
    "print(\"\\nTop 50 most common tokens:\")\n",
    "for token, count in token_counts.most_common(50):\n",
    "    pct = count / total_tokens * 100\n",
    "    print(f\"  {token}: {count:,} ({pct:.3f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency distribution\n",
    "frequencies = list(token_counts.values())\n",
    "\n",
    "print(\"\\nFrequency distribution:\")\n",
    "print(f\"  Min: {min(frequencies):,}\")\n",
    "print(f\"  Max: {max(frequencies):,}\")\n",
    "print(f\"  Mean: {np.mean(frequencies):,.1f}\")\n",
    "print(f\"  Median: {np.median(frequencies):,.1f}\")\n",
    "print(f\"  Std: {np.std(frequencies):,.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Filter by Threshold (EEVE: >= 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEVE threshold\n",
    "MIN_FREQUENCY = 6000\n",
    "\n",
    "# Analyze different thresholds\n",
    "thresholds = [1000, 3000, 6000, 10000, 20000, 50000]\n",
    "\n",
    "print(\"Tokens remaining at different thresholds:\")\n",
    "print(f\"{'Threshold':>10} | {'Tokens':>10} | {'% of Vocab':>12}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    count = sum(1 for freq in frequencies if freq >= threshold)\n",
    "    pct = count / len(frequencies) * 100\n",
    "    marker = \" <-- EEVE\" if threshold == MIN_FREQUENCY else \"\"\n",
    "    print(f\"{threshold:>10,} | {count:>10,} | {pct:>11.1f}%{marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter tokens\n",
    "filtered_tokens = [\n",
    "    token for token, count in token_counts.items()\n",
    "    if count >= MIN_FREQUENCY\n",
    "]\n",
    "\n",
    "print(f\"\\nFiltering with threshold >= {MIN_FREQUENCY:,}\")\n",
    "print(f\"Original tokens: {len(token_counts):,}\")\n",
    "print(f\"Filtered tokens: {len(filtered_tokens):,}\")\n",
    "print(f\"Removed: {len(token_counts) - len(filtered_tokens):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Korean tokens from others\n",
    "def is_korean_token(token):\n",
    "    \"\"\"Check if token contains Korean characters\"\"\"\n",
    "    clean = token.replace(\"▁\", \"\")\n",
    "    return any('가' <= c <= '힣' for c in clean)\n",
    "\n",
    "filtered_korean_tokens = [t for t in filtered_tokens if is_korean_token(t)]\n",
    "filtered_other_tokens = [t for t in filtered_tokens if not is_korean_token(t)]\n",
    "\n",
    "print(f\"\\nFiltered token breakdown:\")\n",
    "print(f\"  Korean tokens: {len(filtered_korean_tokens):,}\")\n",
    "print(f\"  Other tokens: {len(filtered_other_tokens):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Analyze Filtered Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample filtered Korean tokens\n",
    "print(\"Sample filtered Korean tokens (sorted by frequency):\")\n",
    "\n",
    "korean_with_freq = [(t, token_counts[t]) for t in filtered_korean_tokens]\n",
    "korean_with_freq.sort(key=lambda x: -x[1])\n",
    "\n",
    "for token, freq in korean_with_freq[:50]:\n",
    "    print(f\"  {token}: {freq:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize frequency distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Histogram of frequencies (log scale)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist([token_counts[t] for t in filtered_korean_tokens], bins=50, color='#4ECDC4', edgecolor='white')\n",
    "plt.xlabel('Token Frequency')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Frequency Distribution of Filtered Korean Tokens\\n(threshold >= {MIN_FREQUENCY:,})')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Cumulative coverage\n",
    "plt.subplot(1, 2, 2)\n",
    "sorted_freqs = sorted([token_counts[t] for t in filtered_korean_tokens], reverse=True)\n",
    "cumsum = np.cumsum(sorted_freqs) / sum(sorted_freqs) * 100\n",
    "plt.plot(range(len(cumsum)), cumsum, color='#FF6B6B')\n",
    "plt.xlabel('Number of Tokens (ranked by frequency)')\n",
    "plt.ylabel('Cumulative Coverage (%)')\n",
    "plt.title('Token Coverage')\n",
    "plt.axhline(y=90, color='gray', linestyle='--', label='90% coverage')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{MODEL_DIR}/filtered_token_analysis.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Chart saved to {MODEL_DIR}/filtered_token_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check medical term coverage\n",
    "medical_terms_ko = [\n",
    "    \"의사\", \"환자\", \"병원\", \"치료\", \"진단\", \"증상\", \"질병\", \"약물\",\n",
    "    \"수술\", \"검사\", \"혈액\", \"심장\", \"폐\", \"간\", \"신장\", \"뇌\",\n",
    "    \"당뇨\", \"고혈압\", \"암\", \"감염\", \"염증\", \"통증\", \"발열\", \"기침\",\n",
    "]\n",
    "\n",
    "print(\"Medical term coverage check:\")\n",
    "covered = 0\n",
    "not_covered = []\n",
    "\n",
    "for term in medical_terms_ko:\n",
    "    # Check if term exists as token or subtoken\n",
    "    found = False\n",
    "    for token in filtered_korean_tokens:\n",
    "        if term in token.replace(\"▁\", \"\"):\n",
    "            found = True\n",
    "            break\n",
    "    \n",
    "    if found:\n",
    "        covered += 1\n",
    "    else:\n",
    "        not_covered.append(term)\n",
    "\n",
    "print(f\"  Covered: {covered}/{len(medical_terms_ko)} ({covered/len(medical_terms_ko)*100:.1f}%)\")\n",
    "if not_covered:\n",
    "    print(f\"  Not covered: {not_covered}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Save Filtered Token List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtered Korean tokens\n",
    "filtered_tokens_path = f\"{MODEL_DIR}/filtered_korean_tokens.txt\"\n",
    "\n",
    "with open(filtered_tokens_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for token in filtered_korean_tokens:\n",
    "        f.write(f\"{token}\\n\")\n",
    "\n",
    "print(f\"Saved {len(filtered_korean_tokens)} filtered Korean tokens to {filtered_tokens_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save token frequencies for reference\n",
    "frequencies_path = f\"{MODEL_DIR}/token_frequencies.json\"\n",
    "\n",
    "token_freq_data = {\n",
    "    token: count for token, count in token_counts.items()\n",
    "    if token in filtered_tokens\n",
    "}\n",
    "\n",
    "with open(frequencies_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(token_freq_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved token frequencies to {frequencies_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtering summary\n",
    "filter_summary = {\n",
    "    \"min_frequency_threshold\": MIN_FREQUENCY,\n",
    "    \"total_corpus_tokens\": total_tokens,\n",
    "    \"unique_tokens_before\": len(token_counts),\n",
    "    \"filtered_tokens_total\": len(filtered_tokens),\n",
    "    \"filtered_korean_tokens\": len(filtered_korean_tokens),\n",
    "    \"filtered_other_tokens\": len(filtered_other_tokens),\n",
    "    \"files\": {\n",
    "        \"filtered_tokens\": filtered_tokens_path,\n",
    "        \"frequencies\": frequencies_path,\n",
    "    },\n",
    "}\n",
    "\n",
    "summary_path = f\"{MODEL_DIR}/filter_summary.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filter_summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Token Filtering Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(filter_summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Token Filtering Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFiltered Korean tokens: {len(filtered_korean_tokens):,}\")\n",
    "print(f\"Saved to: {filtered_tokens_path}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Run 03_merge_tokenizers.ipynb to merge with MedGemma tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
