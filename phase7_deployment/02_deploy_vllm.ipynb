{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Phase 6.2: Deploy with vLLM\n",
    "\n",
    "Deploy the quantized Korean MedGemma model using vLLM for high-performance inference.\n",
    "\n",
    "## Contents\n",
    "1. Setup\n",
    "2. vLLM Configuration\n",
    "3. Start vLLM Server\n",
    "4. Test API\n",
    "5. Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "print(\"vLLM Deployment Setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Model configuration\n# Primary: Use AWQ quantized model\nMODEL_DIR = \"../models/korean_medgemma_awq\"\n\n# Alternative: Use full precision instruction-tuned model\n# MODEL_DIR = \"../models/instruction_tuned\"\n\n# Alternative: Use expanded model directly\n# MODEL_DIR = \"../models/final/korean_medgemma_expanded\"\n\n# vLLM server configuration\nVLLM_CONFIG = {\n    \"host\": \"0.0.0.0\",\n    \"port\": 8000,\n    \"max_model_len\": 4096,\n    \"gpu_memory_utilization\": 0.9,\n    \"quantization\": \"awq\",  # Set to None for full precision\n    \"dtype\": \"half\",\n}\n\nprint(f\"Model: {MODEL_DIR}\")\nprint(f\"\\nvLLM Configuration:\")\nfor key, value in VLLM_CONFIG.items():\n    print(f\"  {key}: {value}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Create Deployment Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vLLM deployment script\n",
    "deploy_script = f'''#!/bin/bash\n",
    "# Korean MedGemma vLLM Deployment Script\n",
    "\n",
    "MODEL_PATH=\"{os.path.abspath(MODEL_DIR)}\"\n",
    "HOST=\"{VLLM_CONFIG['host']}\"\n",
    "PORT=\"{VLLM_CONFIG['port']}\"\n",
    "\n",
    "echo \"Starting Korean MedGemma vLLM Server...\"\n",
    "echo \"Model: $MODEL_PATH\"\n",
    "echo \"Server: http://$HOST:$PORT\"\n",
    "\n",
    "python -m vllm.entrypoints.openai.api_server \\\\\n",
    "    --model $MODEL_PATH \\\\\n",
    "    --host $HOST \\\\\n",
    "    --port $PORT \\\\\n",
    "    --max-model-len {VLLM_CONFIG['max_model_len']} \\\\\n",
    "    --gpu-memory-utilization {VLLM_CONFIG['gpu_memory_utilization']} \\\\\n",
    "    --dtype {VLLM_CONFIG['dtype']} \\\\\n",
    "    {\"--quantization \" + VLLM_CONFIG['quantization'] if VLLM_CONFIG['quantization'] else \"\"} \\\\\n",
    "    --trust-remote-code\n",
    "'''\n",
    "\n",
    "script_path = \"../scripts/deploy_vllm.sh\"\n",
    "os.makedirs(os.path.dirname(script_path), exist_ok=True)\n",
    "\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write(deploy_script)\n",
    "\n",
    "# Make executable\n",
    "os.chmod(script_path, 0o755)\n",
    "\n",
    "print(f\"Deployment script created: {script_path}\")\n",
    "print(\"\\nScript contents:\")\n",
    "print(deploy_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Start vLLM Server (Run in Terminal)\n",
    "\n",
    "To start the server, run the following command in a terminal:\n",
    "\n",
    "```bash\n",
    "bash ../scripts/deploy_vllm.sh\n",
    "```\n",
    "\n",
    "Or directly with vLLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the vLLM command\n",
    "vllm_command = f\"\"\"\n",
    "python -m vllm.entrypoints.openai.api_server \\\\\n",
    "    --model {os.path.abspath(MODEL_DIR)} \\\\\n",
    "    --host {VLLM_CONFIG['host']} \\\\\n",
    "    --port {VLLM_CONFIG['port']} \\\\\n",
    "    --max-model-len {VLLM_CONFIG['max_model_len']} \\\\\n",
    "    --gpu-memory-utilization {VLLM_CONFIG['gpu_memory_utilization']} \\\\\n",
    "    --dtype {VLLM_CONFIG['dtype']} \\\\\n",
    "    {\"--quantization \" + VLLM_CONFIG['quantization'] if VLLM_CONFIG['quantization'] else \"\"} \\\\\n",
    "    --trust-remote-code\n",
    "\"\"\"\n",
    "\n",
    "print(\"Run this command in a terminal to start the server:\")\n",
    "print(vllm_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Test API (After Server is Running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API endpoint\n",
    "API_URL = f\"http://localhost:{VLLM_CONFIG['port']}/v1/chat/completions\"\n",
    "\n",
    "def chat_with_model(messages, max_tokens=256, temperature=0.7):\n",
    "    \"\"\"Send chat request to vLLM server\"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL_DIR,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(API_URL, json=payload, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return {\"error\": \"Server not running. Start the vLLM server first.\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "print(f\"API endpoint: {API_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Korean medical question\n",
    "print(\"Testing Korean medical question...\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"당신은 한국어 의료 전문 AI 어시스턴트입니다.\"},\n",
    "    {\"role\": \"user\", \"content\": \"고혈압의 주요 증상과 치료법에 대해 설명해주세요.\"}\n",
    "]\n",
    "\n",
    "response = chat_with_model(messages)\n",
    "\n",
    "if \"error\" in response:\n",
    "    print(f\"Error: {response['error']}\")\n",
    "    print(\"\\nMake sure the vLLM server is running.\")\n",
    "else:\n",
    "    print(\"\\nResponse:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test English medical question\n",
    "print(\"Testing English medical question...\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a medical AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the symptoms and treatment options for Type 2 Diabetes?\"}\n",
    "]\n",
    "\n",
    "response = chat_with_model(messages)\n",
    "\n",
    "if \"error\" in response:\n",
    "    print(f\"Error: {response['error']}\")\n",
    "else:\n",
    "    print(\"\\nResponse:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Python Client Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Python client example\n",
    "client_code = f'''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Korean MedGemma Python Client Example\n",
    "\n",
    "Usage:\n",
    "    python korean_medgemma_client.py \"고혈압의 증상은 무엇인가요?\"\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "API_URL = \"http://localhost:{VLLM_CONFIG['port']}/v1/chat/completions\"\n",
    "MODEL_NAME = \"{MODEL_DIR}\"\n",
    "\n",
    "def ask_medical_question(question, language=\"ko\"):\n",
    "    \"\"\"\n",
    "    Ask a medical question to Korean MedGemma.\n",
    "    \n",
    "    Args:\n",
    "        question: The medical question\n",
    "        language: \"ko\" for Korean, \"en\" for English\n",
    "    \n",
    "    Returns:\n",
    "        The model\\'s response\n",
    "    \"\"\"\n",
    "    \n",
    "    if language == \"ko\":\n",
    "        system_prompt = \"당신은 한국어 의료 전문 AI 어시스턴트입니다. 정확하고 도움이 되는 의료 정보를 제공하세요.\"\n",
    "    else:\n",
    "        system_prompt = \"You are a medical AI assistant. Provide accurate and helpful medical information.\"\n",
    "    \n",
    "    payload = {{\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {{\"role\": \"system\", \"content\": system_prompt}},\n",
    "            {{\"role\": \"user\", \"content\": question}}\n",
    "        ],\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.7,\n",
    "    }}\n",
    "    \n",
    "    response = requests.post(API_URL, json=payload)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) > 1:\n",
    "        question = \" \".join(sys.argv[1:])\n",
    "    else:\n",
    "        question = \"고혈압의 증상과 치료법은 무엇인가요?\"\n",
    "    \n",
    "    print(f\"Question: {{question}}\")\n",
    "    print(\"\\nAnswer:\")\n",
    "    \n",
    "    try:\n",
    "        answer = ask_medical_question(question)\n",
    "        print(answer)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {{e}}\")\n",
    "        print(\"Make sure the vLLM server is running.\")\n",
    "'''\n",
    "\n",
    "client_path = \"../scripts/korean_medgemma_client.py\"\n",
    "with open(client_path, \"w\") as f:\n",
    "    f.write(client_code)\n",
    "\n",
    "print(f\"Python client created: {client_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Docker Deployment (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dockerfile for deployment\n",
    "dockerfile = f'''\n",
    "# Korean MedGemma Docker Deployment\n",
    "FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04\n",
    "\n",
    "# Install Python and dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    python3 python3-pip git && \\\\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install vLLM\n",
    "RUN pip3 install vllm autoawq\n",
    "\n",
    "# Copy model (or mount as volume)\n",
    "# COPY models/korean_medgemma_awq /app/model\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run vLLM server\n",
    "CMD [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\", \\\\\n",
    "     \"--model\", \"/app/model\", \\\\\n",
    "     \"--host\", \"0.0.0.0\", \\\\\n",
    "     \"--port\", \"8000\", \\\\\n",
    "     \"--quantization\", \"awq\", \\\\\n",
    "     \"--trust-remote-code\"]\n",
    "'''\n",
    "\n",
    "dockerfile_path = \"../Dockerfile\"\n",
    "with open(dockerfile_path, \"w\") as f:\n",
    "    f.write(dockerfile)\n",
    "\n",
    "print(f\"Dockerfile created: {dockerfile_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create docker-compose file\n",
    "docker_compose = f'''\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  korean-medgemma:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    volumes:\n",
    "      - ./models/korean_medgemma_awq:/app/model:ro\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    environment:\n",
    "      - CUDA_VISIBLE_DEVICES=0\n",
    "    restart: unless-stopped\n",
    "'''\n",
    "\n",
    "compose_path = \"../docker-compose.yml\"\n",
    "with open(compose_path, \"w\") as f:\n",
    "    f.write(docker_compose)\n",
    "\n",
    "print(f\"Docker Compose created: {compose_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Korean MedGemma Deployment Complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "Deployment files created:\n",
    "  - Deployment script: {script_path}\n",
    "  - Python client: {client_path}\n",
    "  - Dockerfile: {dockerfile_path}\n",
    "  - Docker Compose: {compose_path}\n",
    "\n",
    "To start the server:\n",
    "  bash {script_path}\n",
    "\n",
    "Or with Docker:\n",
    "  docker-compose up\n",
    "\n",
    "API endpoint: http://localhost:8000/v1/chat/completions\n",
    "\n",
    "Example curl command:\n",
    "  curl -X POST http://localhost:8000/v1/chat/completions \\\\\n",
    "    -H \"Content-Type: application/json\" \\\\\n",
    "    -d '{{\n",
    "      \"model\": \"{MODEL_DIR}\",\n",
    "      \"messages\": [\n",
    "        {{\"role\": \"system\", \"content\": \"당신은 의료 AI 어시스턴트입니다.\"}},\n",
    "        {{\"role\": \"user\", \"content\": \"고혈압이란 무엇인가요?\"}}\n",
    "      ]\n",
    "    }}'\n",
    "\n",
    "Project complete! You now have a Korean-adapted MedGemma model\n",
    "ready for production deployment.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}