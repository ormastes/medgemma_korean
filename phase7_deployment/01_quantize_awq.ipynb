{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Phase 6.1: AWQ Quantization for Deployment\n",
    "\n",
    "Quantize the model to 4-bit AWQ format for efficient inference.\n",
    "\n",
    "## Contents\n",
    "1. Setup\n",
    "2. Load Model\n",
    "3. Apply AWQ Quantization\n",
    "4. Verify Quantized Model\n",
    "5. Save Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n",
    "from awq import AutoAWQForCausalLM\n",
    "import json\n",
    "\n",
    "# GPU setup\n",
    "from config.gpu_utils import setup_gpu, print_memory_usage, clear_memory\n",
    "device = setup_gpu()\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Directories\n# Primary: Use instruction-tuned model (based on expanded model)\nMODEL_DIR = \"../models/instruction_tuned\"\n\n# Alternative: Use expanded model directly (no instruction tuning)\n# MODEL_DIR = \"../models/final/korean_medgemma_expanded\"\n\n# Legacy (non-expanded):\n# MODEL_DIR = \"../models/final/korean_medgemma\"\n\nOUTPUT_DIR = \"../models/korean_medgemma_awq\"\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"Input model: {MODEL_DIR}\")\nprint(f\"Output dir: {OUTPUT_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. AWQ Quantization Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWQ configuration\n",
    "AWQ_CONFIG = {\n",
    "    \"w_bit\": 4,  # 4-bit quantization\n",
    "    \"q_group_size\": 128,  # Group size for quantization\n",
    "    \"zero_point\": True,  # Use zero point\n",
    "    \"version\": \"GEMM\",  # Optimized for inference\n",
    "}\n",
    "\n",
    "print(\"AWQ Configuration:\")\n",
    "for key, value in AWQ_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Model for Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model using AutoAWQ\n",
    "print(\"Loading model for AWQ quantization...\")\n",
    "\n",
    "model = AutoAWQForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    trust_remote_code=True,\n",
    "    safetensors=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "\n",
    "print(\"Model loaded!\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Prepare Calibration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare calibration data for AWQ\n",
    "# AWQ needs sample data to calibrate quantization\n",
    "\n",
    "calibration_texts = [\n",
    "    # Korean medical texts\n",
    "    \"고혈압은 혈압이 정상보다 높은 상태를 말합니다. 수축기 혈압이 140mmHg 이상이거나 이완기 혈압이 90mmHg 이상인 경우를 고혈압으로 정의합니다.\",\n",
    "    \"당뇨병은 인슐린 분비나 작용에 문제가 생겨 혈당이 높아지는 대사 질환입니다. 제1형 당뇨병과 제2형 당뇨병으로 분류됩니다.\",\n",
    "    \"폐렴은 폐에 염증이 생기는 질환으로, 세균, 바이러스, 곰팡이 등이 원인이 될 수 있습니다. 기침, 발열, 호흡곤란이 주요 증상입니다.\",\n",
    "    \"심근경색은 심장 근육에 혈액 공급이 차단되어 발생하는 응급 상황입니다. 가슴 통증, 호흡곤란, 식은땀이 주요 증상입니다.\",\n",
    "    \"뇌졸중은 뇌혈관이 막히거나 터져서 발생하는 질환입니다. 갑작스러운 마비, 언어 장애, 두통이 나타날 수 있습니다.\",\n",
    "    \n",
    "    # English medical texts\n",
    "    \"Hypertension is defined as blood pressure consistently above 140/90 mmHg. It is a major risk factor for heart disease, stroke, and kidney disease.\",\n",
    "    \"Diabetes mellitus is a metabolic disorder characterized by elevated blood glucose levels. Type 2 diabetes is the most common form.\",\n",
    "    \"Pneumonia is an infection that inflames the air sacs in one or both lungs. Symptoms include cough, fever, and difficulty breathing.\",\n",
    "    \"Myocardial infarction occurs when blood flow to the heart muscle is blocked. Prompt treatment is essential to minimize heart damage.\",\n",
    "    \"Stroke is a medical emergency caused by disrupted blood supply to the brain. Symptoms include sudden weakness and speech difficulties.\",\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(calibration_texts)} calibration texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Apply AWQ Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize model\n",
    "print(\"\\nApplying AWQ quantization...\")\n",
    "print(\"This may take 10-30 minutes depending on model size.\")\n",
    "\n",
    "quant_config = {\n",
    "    \"zero_point\": AWQ_CONFIG[\"zero_point\"],\n",
    "    \"q_group_size\": AWQ_CONFIG[\"q_group_size\"],\n",
    "    \"w_bit\": AWQ_CONFIG[\"w_bit\"],\n",
    "    \"version\": AWQ_CONFIG[\"version\"],\n",
    "}\n",
    "\n",
    "model.quantize(\n",
    "    tokenizer,\n",
    "    quant_config=quant_config,\n",
    "    calib_data=calibration_texts,\n",
    ")\n",
    "\n",
    "print(\"\\nQuantization complete!\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Verify Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test quantized model\n",
    "print(\"\\nTesting quantized model...\")\n",
    "\n",
    "test_prompts = [\n",
    "    \"고혈압의 증상과 치료법은 무엇인가요?\",\n",
    "    \"What are the symptoms of diabetes?\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    formatted_prompt = f\"\"\"<|im_start|>system\n",
    "당신은 의료 AI 어시스턴트입니다.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nQ: {prompt}\")\n",
    "    print(f\"A: {response[len(formatted_prompt):][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Save Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save quantized model\n",
    "print(f\"\\nSaving quantized model to {OUTPUT_DIR}...\")\n",
    "\n",
    "model.save_quantized(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Quantized model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save quantization info\n",
    "quant_info = {\n",
    "    \"source_model\": MODEL_DIR,\n",
    "    \"quantization_method\": \"AWQ\",\n",
    "    \"config\": AWQ_CONFIG,\n",
    "    \"calibration_samples\": len(calibration_texts),\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/quantization_info.json\", \"w\") as f:\n",
    "    json.dump(quant_info, f, indent=2)\n",
    "\n",
    "print(\"Quantization info saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model size\n",
    "import os\n",
    "\n",
    "def get_folder_size(folder):\n",
    "    total = 0\n",
    "    for path, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            fp = os.path.join(path, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total\n",
    "\n",
    "original_size = get_folder_size(MODEL_DIR) / (1024**3)\n",
    "quantized_size = get_folder_size(OUTPUT_DIR) / (1024**3)\n",
    "\n",
    "print(f\"\\nModel size comparison:\")\n",
    "print(f\"  Original: {original_size:.2f} GB\")\n",
    "print(f\"  Quantized: {quantized_size:.2f} GB\")\n",
    "print(f\"  Compression: {original_size / quantized_size:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AWQ Quantization Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nQuantized model saved to: {OUTPUT_DIR}\")\n",
    "print(f\"Compression ratio: {original_size / quantized_size:.1f}x\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  Run 02_deploy_vllm.ipynb to deploy with vLLM\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}