{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Phase 4: Korean Medical Instruction Tuning\n",
    "\n",
    "Fine-tune the Korean-adapted model on medical instruction data.\n",
    "\n",
    "## Purpose\n",
    "- Train on Korean medical QA format\n",
    "- Enable instruction-following capabilities\n",
    "- Use KorMedMCQA and other instruction data\n",
    "\n",
    "## Contents\n",
    "1. Setup and Configuration\n",
    "2. Load Model from Phase 3\n",
    "3. Load Instruction Data\n",
    "4. Training\n",
    "5. Test Generation\n",
    "6. Save Instruction-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_from_disk\n",
    "import json\n",
    "\n",
    "# GPU setup\n",
    "from config.gpu_utils import setup_gpu, print_memory_usage, clear_memory\n",
    "device = setup_gpu()\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Directories\n# Use expanded model from Phase 3 Stage 7 (hybrid: identity layers + QLoRA)\nBASE_MODEL_DIR = \"../models/final/korean_medgemma_expanded\"\n\n# Alternative: Use Stage 7 cooldown checkpoint directly\n# BASE_MODEL_DIR = \"../models/staged_training/stage7_cooldown\"\n\n# Legacy (non-expanded model):\n# BASE_MODEL_DIR = \"../models/final/korean_medgemma\"\n\nDATA_DIR = \"../data/processed\"\nOUTPUT_DIR = \"../models/instruction_tuned\"\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"Base model: {BASE_MODEL_DIR}\")\nprint(f\"Output dir: {OUTPUT_DIR}\")\nprint(\"\\nNote: Using hybrid expanded model with +2 identity layers\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction tuning configuration\n",
    "CONFIG = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"max_seq_length\": 2048,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    # LoRA config for instruction tuning\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 64,\n",
    "    \"lora_dropout\": 0.05,\n",
    "}\n",
    "\n",
    "print(\"Instruction Tuning Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"BitsAndBytes config created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"\\nLoading Korean MedGemma model...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_DIR,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_DIR)\n",
    "\n",
    "# Ensure padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"Model prepared for k-bit training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA for instruction tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\nLoRA applied for instruction tuning\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load Instruction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load instruction dataset\n",
    "instruction_data_path = f\"{DATA_DIR}/korean_medical_instruction\"\n",
    "\n",
    "if os.path.exists(instruction_data_path):\n",
    "    dataset = load_from_disk(instruction_data_path)\n",
    "    print(f\"Loaded instruction dataset: {dataset}\")\n",
    "else:\n",
    "    print(f\"Dataset not found at {instruction_data_path}\")\n",
    "    print(\"Run Phase 0 notebooks to prepare data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview instruction data\n",
    "print(\"Sample instruction:\")\n",
    "print(dataset[\"train\"][0][\"text\"][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_grad_norm=0.3,\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"] if \"validation\" in dataset else None,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "print(\"SFT Trainer created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Starting Instruction Tuning\")\n",
    "print(\"=\" * 60)\n",
    "print_memory_usage()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the instruction-tuned model\n",
    "test_prompts = [\n",
    "    \"\"\"<|im_start|>system\n",
    "당신은 한국어 의료 전문 AI 어시스턴트입니다.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "고혈압의 주요 증상과 치료법에 대해 설명해주세요.\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\",\n",
    "    \"\"\"<|im_start|>system\n",
    "당신은 한국어 의료 전문 AI 어시스턴트입니다.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "당뇨병 환자가 주의해야 할 식이요법은 무엇인가요?\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\",\n",
    "]\n",
    "\n",
    "print(\"Testing instruction-tuned model:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    print(f\"\\n--- Test {i+1} ---\")\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save instruction-tuned model\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"\\nInstruction-tuned model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training info\n",
    "training_info = {\n",
    "    \"phase\": \"instruction_tuning\",\n",
    "    \"base_model\": BASE_MODEL_DIR,\n",
    "    \"config\": CONFIG,\n",
    "    \"train_samples\": len(dataset[\"train\"]),\n",
    "    \"eval_samples\": len(dataset[\"validation\"]) if \"validation\" in dataset else 0,\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/training_info.json\", \"w\") as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(\"Training info saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Phase 4 Complete: Instruction Tuning Done!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Run phase5_evaluation/01_evaluate_korean.ipynb\")\n",
    "print(\"  2. Run phase5_evaluation/02_evaluate_english.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}