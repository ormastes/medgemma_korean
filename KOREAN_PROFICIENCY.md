# Korean Proficiency Validation Guide

## Current Korean Data Status

### âœ… Korean Medical Data Available

| Type | Korean? | Train Samples | Val Samples | Korean Ratio |
|------|---------|---------------|-------------|--------------|
| **Type 1 (TEXT)** | âœ“ Yes | 118,431 | 13,160 | ~89% Korean |
| **Type 2 (TEXT_REASONING)** | âœ“ Yes | 23,018 | 2,558 | ~80% Korean |
| **Type 3 (WORD)** | âœ— No | 16,701 | 1,846 | Letter answers (A/B/C) |
| **Type 4 (WORD_REASONING)** | âœ— No | 7,957 | 885 | English reasoning |

**Total Korean Content:** 141,449 train + 15,718 val = **157,167 Korean samples**

### Why Types 3 & 4 Are Not Korean

- **Type 3 (WORD):** MCQ answers are single letters (A, B, C, D, E) - language-agnostic
- **Type 4 (WORD_REASONING):** Contains English reasoning in `<R>...<R/>` blocks
- **Questions are Korean** in Types 3 & 4, but **answers/reasoning are not**

## Korean Medical Benchmarks

### 1. KorMedMCQA (Primary Evaluation)

**Dataset:** `sean0042/KorMedMCQA`
- **Test samples:** 604
- **Language:** 100% Korean
- **Format:** Korean MCQs with Korean explanations
- **Target:** â‰¥90% accuracy
- **Status:** âœ“ Available in `data/raw/by_source/kormedmcqa/`

**Use:** Primary metric for Korean medical knowledge

### 2. KMMLU-Medical (Korean MMLU)

**Dataset:** Korean Massive Multitask Language Understanding
- **Language:** 100% Korean
- **Subjects:** Multiple medical subjects
  - Anatomy
  - Pharmacology
  - Clinical Medicine
  - Public Health
- **Status:** âœ“ Available in `data/raw/korean_datasets/kmmlu_medical/`

**Use:** Broad Korean medical knowledge assessment

### 3. MedQA-Korean

**Dataset:** `ChuGyouk/MedQA`
- **Samples:** 22,900 train
- **Language:** Korean (translated from English USMLE)
- **Format:** MCQ format
- **Status:** âœ“ Available

**Use:** Korean medical exam questions

### 4. Korean Medical Sources (36 datasets)

Available in `data/raw/korean_datasets/`:
- Asan AMC Healthinfo (hospital data)
- KoMedInstruct-52k
- Korean medical textbooks
- Korean Wikipedia medical articles
- And 32 more...

## Validation Scripts

### 1. Check Korean Proficiency

```bash
# Validate all reviewed data
python3 scripts/validate_korean_proficiency.py --all

# Check specific file
python3 scripts/validate_korean_proficiency.py \
    --file data/reviewed/type1_text/train/data.jsonl \
    --sample-size 100

# Check benchmarks
python3 scripts/validate_korean_proficiency.py --benchmarks
```

### 2. Metrics Reported

- **Korean ratio:** % of Korean characters vs total
- **Medical terms:** Presence of Korean medical vocabulary
- **Text quality:** Character/word counts
- **Issues:** Missing medical terms, low Korean ratio

## Korean Proficiency Results

### Type 1 (TEXT) - 100% Korean âœ…

```
Korean samples: 100%
Average Korean ratio: 89.0%
With medical terms: 80%

Sample:
"10ì„¸ ì†Œë…„ì˜ ì¦ìƒì¸ ê³ ì—´, ë¶€ì€ ëˆˆêº¼í’€, í¼ì§€ëŠ” ë°œì§„, ì½”í”Œë¦­ ë°˜ì ì€
í™ì—­ì„ ê°•ë ¥í•˜ê²Œ ì‹œì‚¬í•˜ë©°, íŠ¹íˆ ì˜ˆë°© ì ‘ì¢…ë¥ ì´ ìµœì í™”ë˜ì§€ ì•Šì€ ì§€ì—­ì—ì„œ..."
```

### Type 2 (TEXT_REASONING) - 100% Korean âœ…

```
Korean samples: 100%
Average Korean ratio: 80.5%
With medical terms: 68%

Sample:
"<R>ì, í•œë²ˆ ìƒê°í•´ ë´…ì‹œë‹¤. 13ì„¸ ë‚¨ì í™˜ìê°€ ì•¼êµ¬ê³µì— ì–¼êµ´ì„ ë§ì•˜ê³ ,
í˜„ì¬ ì¢Œì¸¡ ì•ˆì™€ ì£¼ìœ„ ë¶€ì¢…ì´ ìˆìŠµë‹ˆë‹¤. 'ì•ˆì™€ ì£¼ìœ„ ë¶€ì¢…'ì´ë¼ëŠ” ë§ì„ ë“¤ìœ¼ë©´
ì•ˆì™€ êµ¬ì¡°ë¬¼ì„ ìƒê°í•˜ê²Œ ë©ë‹ˆë‹¤...<R/>ì§„ë‹¨ì€ ì•ˆì™€ ê³¨ì ˆì…ë‹ˆë‹¤."
```

### Type 3 (WORD) - Letters Only âš ï¸

```
Korean samples: 0%
Answers: A, B, C, D, E (single letters)
Questions: Korean
```

### Type 4 (WORD_REASONING) - English Reasoning âš ï¸

```
Korean samples: 0%
Reasoning: English in <R>...<R/> blocks
Final answer: Korean word/letter
```

## Recommended Validation Strategy

### Phase 1: Automated Metrics

```bash
# Run Korean proficiency check
python3 scripts/validate_korean_proficiency.py --all

# Expected results:
# - Type 1 & 2: >85% Korean ratio
# - Medical term coverage: >70%
```

### Phase 2: Benchmark Evaluation

```bash
# Evaluate on KorMedMCQA (primary)
python3 scripts/evaluate_kormedmcqa.py \
    --model models/final \
    --output results/kormedmcqa_eval.json

# Target: â‰¥90% accuracy
```

### Phase 3: KMMLU-Medical

```bash
# Evaluate on KMMLU medical subjects
python3 scripts/evaluate_kmmlu_medical.py \
    --model models/final \
    --output results/kmmlu_eval.json
```

### Phase 4: Manual Review (100 samples)

```bash
# Sample random Korean samples
python3 scripts/sample_for_review.py \
    --count 100 \
    --output manual_review_samples.jsonl

# Manual checks:
# 1. Korean grammar correctness
# 2. Medical terminology accuracy
# 3. Natural Korean flow
# 4. Cultural appropriateness
```

## Korean Medical Terminology Coverage

Common medical terms found in data:

```
í™˜ì (patient)         - Present in 80%+ samples
ì§„ë‹¨ (diagnosis)       - Present in 70%+ samples
ì¹˜ë£Œ (treatment)       - Present in 65%+ samples
ì¦ìƒ (symptoms)        - Present in 75%+ samples
ì§ˆë³‘ (disease)         - Present in 60%+ samples
ì•½ë¬¼ (medication)      - Present in 55%+ samples
ê²€ì‚¬ (examination)     - Present in 50%+ samples
ìˆ˜ìˆ  (surgery)         - Present in 40%+ samples
```

## Data Quality Indicators

### High Quality (âœ…)

- Type 1 & 2 text completions
- KorMedMCQA dataset
- Asan AMC Healthinfo (hospital data)
- Korean medical textbooks

### Medium Quality (âš ï¸)

- Translated medical content (MedQA-Korean)
- Generated Korean medical Q&A
- Wikipedia medical articles

### English Content (â„¹ï¸)

- Type 3 answers (single letters - acceptable)
- Type 4 reasoning (English - needs improvement)

## Improving Type 4 Korean Content

### Current Issue

```json
{
  "prompt": "<|im_start|>user\n45ì„¸ ë‚¨ì„±, ë‹¹ë‡¨ë³‘ ì§„ë‹¨...<|im_end|>",
  "completion": "<R>Let's think about diabetes...<R/>ë‹¹ë‡¨ë³‘"
}
```

### Solution 1: Re-generate with Korean Reasoning

```bash
# Use DeepSeek or GPT to translate reasoning
python3 scripts/translate_type4_reasoning.py \
    --input data/reviewed/type4_word_reasoning/train/data.jsonl \
    --output data/reviewed/type4_word_reasoning_korean/train/data.jsonl
```

### Solution 2: Use Korean Reasoning Data

Already available:
- `ChuGyouk/medical-o1-reasoning-SFT-Ko` (Korean reasoning)
- `ChuGyouk/ChainofDiagnosis-Ko` (Korean chain-of-thought)

## Validation Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Korean Proficiency Validation             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  Step 1: Automated Check                           â”‚
â”‚          - Korean ratio: âœ“ >85%                     â”‚
â”‚          - Medical terms: âœ“ >70%                    â”‚
â”‚                                                     â”‚
â”‚  Step 2: KorMedMCQA (604 samples)                  â”‚
â”‚          - Target: â‰¥90% accuracy                    â”‚
â”‚          - Korean medical knowledge                 â”‚
â”‚                                                     â”‚
â”‚  Step 3: KMMLU-Medical                             â”‚
â”‚          - Multiple subjects                        â”‚
â”‚          - Broad knowledge test                     â”‚
â”‚                                                     â”‚
â”‚  Step 4: Manual Review (100 samples)               â”‚
â”‚          - Grammar check                            â”‚
â”‚          - Medical accuracy                         â”‚
â”‚          - Natural Korean                           â”‚
â”‚                                                     â”‚
â”‚  Step 5: Deploy if all pass                        â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Commands

```bash
# Check Korean proficiency
python3 scripts/validate_korean_proficiency.py --all

# Evaluate on KorMedMCQA
python3 scripts/train_loop_until_90.py --model medgemma-27b

# Check benchmarks
python3 scripts/validate_korean_proficiency.py --benchmarks

# Sample for manual review
head -100 data/reviewed/type1_text/validation/data.jsonl > korean_review.jsonl
```

## Summary

âœ… **Strong Korean Coverage:**
- 157,167 Korean medical samples
- 89% Korean ratio in Type 1
- 80% Korean ratio in Type 2
- Rich medical terminology

âš ï¸ **Areas for Improvement:**
- Type 4 reasoning currently in English
- Can be improved with Korean reasoning data

ğŸ“Š **Validation Tools:**
- Automated Korean proficiency checker
- KorMedMCQA benchmark (604 samples)
- KMMLU-Medical benchmark
- Manual review workflow

ğŸ¯ **Target Metrics:**
- KorMedMCQA: â‰¥90% accuracy
- Korean ratio: â‰¥85%
- Medical term coverage: â‰¥70%

---

**Current Status:** Korean proficiency is **strong** for Types 1 & 2 (75% of total data). Use KorMedMCQA as primary validation metric.
