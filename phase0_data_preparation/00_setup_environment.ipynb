{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 0.0: Environment Setup\n",
    "\n",
    "Setup PyTorch, CUDA, and configure RTX A5000 as default GPU for training.\n",
    "\n",
    "## Contents\n",
    "1. Check system and GPU info\n",
    "2. Install required packages\n",
    "3. Configure default GPU (A5000)\n",
    "4. Verify setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system info\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"System Information\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Processor: {platform.processor()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available GPUs\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List GPU details\n",
    "!nvidia-smi --query-gpu=index,name,memory.total,memory.free,driver_version --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core ML packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers and related\n",
    "!pip install transformers>=4.40.0\n",
    "!pip install datasets>=2.18.0\n",
    "!pip install accelerate>=0.27.0\n",
    "!pip install peft>=0.10.0\n",
    "!pip install trl>=0.8.0\n",
    "!pip install bitsandbytes>=0.43.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer training\n",
    "!pip install sentencepiece>=0.2.0\n",
    "!pip install tokenizers>=0.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization\n",
    "!pip install deepspeed>=0.14.0\n",
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation and utilities\n",
    "!pip install lm-eval>=0.4.0\n",
    "!pip install wandb\n",
    "!pip install tensorboard\n",
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment\n",
    "!pip install vllm>=0.4.0\n",
    "!pip install autoawq>=0.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Default GPU (RTX A5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def find_a5000_gpu():\n",
    "    \"\"\"Find RTX A5000 GPU index\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available!\")\n",
    "        return None\n",
    "    \n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f\"Found {n_gpus} GPU(s):\")\n",
    "    \n",
    "    a5000_idx = None\n",
    "    \n",
    "    for i in range(n_gpus):\n",
    "        name = torch.cuda.get_device_name(i)\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        memory_gb = props.total_memory / (1024**3)\n",
    "        \n",
    "        print(f\"  GPU {i}: {name} ({memory_gb:.1f} GB)\")\n",
    "        \n",
    "        # Check for A5000 (48GB or 24GB variant)\n",
    "        if \"A5000\" in name or \"RTX A5000\" in name:\n",
    "            a5000_idx = i\n",
    "            print(f\"    -> Found RTX A5000 at index {i}\")\n",
    "    \n",
    "    return a5000_idx\n",
    "\n",
    "a5000_idx = find_a5000_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_default_gpu(gpu_idx=None):\n",
    "    \"\"\"Set default GPU for PyTorch\"\"\"\n",
    "    \n",
    "    if gpu_idx is None:\n",
    "        # Try to find A5000, otherwise use GPU 0\n",
    "        gpu_idx = find_a5000_gpu()\n",
    "        if gpu_idx is None:\n",
    "            gpu_idx = 0\n",
    "            print(f\"A5000 not found, using GPU {gpu_idx}\")\n",
    "    \n",
    "    # Set CUDA_VISIBLE_DEVICES\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_idx)\n",
    "    \n",
    "    # Set default device\n",
    "    torch.cuda.set_device(0)  # After CUDA_VISIBLE_DEVICES, it becomes index 0\n",
    "    \n",
    "    print(f\"\\nDefault GPU set to: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'not set')}\")\n",
    "    \n",
    "    return gpu_idx\n",
    "\n",
    "# Set A5000 as default (or specify index manually)\n",
    "# If you have multiple GPUs and know the A5000 index, use: set_default_gpu(1)\n",
    "DEFAULT_GPU = set_default_gpu(a5000_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GPU config for other notebooks\n",
    "import json\n",
    "\n",
    "gpu_config = {\n",
    "    \"default_gpu_idx\": DEFAULT_GPU,\n",
    "    \"gpu_name\": torch.cuda.get_device_name(0),\n",
    "    \"total_memory_gb\": torch.cuda.get_device_properties(0).total_memory / (1024**3),\n",
    "    \"cuda_version\": torch.version.cuda,\n",
    "    \"pytorch_version\": torch.__version__,\n",
    "}\n",
    "\n",
    "config_path = \"../config/gpu_config.json\"\n",
    "os.makedirs(\"../config\", exist_ok=True)\n",
    "\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(gpu_config, f, indent=2)\n",
    "\n",
    "print(f\"GPU config saved to {config_path}\")\n",
    "print(json.dumps(gpu_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create GPU Setup Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper module for other notebooks\n",
    "helper_code = '''\"\"\"GPU Setup Helper for Korean MedGemma Training\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "def setup_gpu(config_path=\"../config/gpu_config.json\"):\n",
    "    \"\"\"Load GPU config and set as default\"\"\"\n",
    "    \n",
    "    # Load config if exists\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        gpu_idx = config.get(\"default_gpu_idx\", 0)\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_idx)\n",
    "    \n",
    "    # Verify CUDA\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA not available!\")\n",
    "    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "def get_memory_info():\n",
    "    \"\"\"Get current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / (1024**3)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024**3)\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        \n",
    "        return {\n",
    "            \"allocated_gb\": allocated,\n",
    "            \"reserved_gb\": reserved,\n",
    "            \"total_gb\": total,\n",
    "            \"free_gb\": total - reserved,\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    info = get_memory_info()\n",
    "    if info:\n",
    "        print(f\"GPU Memory: {info[\\'allocated_gb\\']:.2f} GB allocated, \"\n",
    "              f\"{info[\\'free_gb\\']:.2f} GB free / {info[\\'total_gb\\']:.1f} GB total\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory cache\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"GPU memory cache cleared\")\n",
    "'''\n",
    "\n",
    "os.makedirs(\"../config\", exist_ok=True)\n",
    "with open(\"../config/gpu_utils.py\", \"w\") as f:\n",
    "    f.write(helper_code)\n",
    "\n",
    "print(\"Created ../config/gpu_utils.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test imports\n",
    "print(\"Testing imports...\")\n",
    "\n",
    "import torch\n",
    "print(f\"  torch: {torch.__version__}\")\n",
    "\n",
    "import transformers\n",
    "print(f\"  transformers: {transformers.__version__}\")\n",
    "\n",
    "import datasets\n",
    "print(f\"  datasets: {datasets.__version__}\")\n",
    "\n",
    "import peft\n",
    "print(f\"  peft: {peft.__version__}\")\n",
    "\n",
    "import accelerate\n",
    "print(f\"  accelerate: {accelerate.__version__}\")\n",
    "\n",
    "import sentencepiece\n",
    "print(f\"  sentencepiece: {sentencepiece.__version__}\")\n",
    "\n",
    "try:\n",
    "    import bitsandbytes\n",
    "    print(f\"  bitsandbytes: {bitsandbytes.__version__}\")\n",
    "except:\n",
    "    print(\"  bitsandbytes: not installed\")\n",
    "\n",
    "print(\"\\nAll imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CUDA operations\n",
    "print(\"Testing CUDA operations...\")\n",
    "\n",
    "# Create tensor on GPU\n",
    "x = torch.randn(1000, 1000, device=\"cuda\")\n",
    "y = torch.randn(1000, 1000, device=\"cuda\")\n",
    "\n",
    "# Matrix multiplication\n",
    "z = torch.matmul(x, y)\n",
    "\n",
    "print(f\"  Matrix multiplication on GPU: {z.shape}\")\n",
    "print(f\"  Result device: {z.device}\")\n",
    "\n",
    "# Clean up\n",
    "del x, y, z\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nCUDA operations successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test HuggingFace Hub access\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "print(\"Testing HuggingFace Hub access...\")\n",
    "api = HfApi()\n",
    "\n",
    "# Check if we can access datasets\n",
    "try:\n",
    "    info = api.dataset_info(\"sean0042/KorMedMCQA\")\n",
    "    print(f\"  KorMedMCQA dataset: {info.id}\")\n",
    "    print(f\"  Downloads: {info.downloads}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Warning: Could not access HuggingFace Hub: {e}\")\n",
    "\n",
    "print(\"\\nHuggingFace Hub access test complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"Environment Setup Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"\\nConfig saved to: ../config/gpu_config.json\")\n",
    "print(f\"Helper module: ../config/gpu_utils.py\")\n",
    "print(\"\\nReady for Phase 0.1: Dataset Research!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
