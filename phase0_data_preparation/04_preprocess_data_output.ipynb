{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e44ebf9",
   "metadata": {
    "papermill": {
     "duration": 0.006883,
     "end_time": "2025-12-05T09:29:06.313748",
     "exception": false,
     "start_time": "2025-12-05T09:29:06.306865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Phase 0.4: Preprocess and Format Training Data\n",
    "\n",
    "Prepare and format all collected data for training stages.\n",
    "\n",
    "## Contents\n",
    "1. Load Collected Data\n",
    "2. Format for Language Modeling (Stage 1-5)\n",
    "3. Format for Instruction Tuning (Stage 6-7)\n",
    "4. Create Mixed Dataset (Korean + English)\n",
    "5. Create Evaluation Splits\n",
    "6. Save All Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed54e4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:06.326805Z",
     "iopub.status.busy": "2025-12-05T09:29:06.326511Z",
     "iopub.status.idle": "2025-12-05T09:29:08.388351Z",
     "shell.execute_reply": "2025-12-05T09:29:08.387396Z"
    },
    "papermill": {
     "duration": 2.070196,
     "end_time": "2025-12-05T09:29:08.389777",
     "exception": false,
     "start_time": "2025-12-05T09:29:06.319581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ormastes/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data: ../data/raw\n",
      "Processed data: ../data/processed\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from datasets import load_from_disk, Dataset, DatasetDict, concatenate_datasets\n",
    "import random\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"../data\"\n",
    "RAW_DIR = f\"{DATA_DIR}/raw\"\n",
    "PROCESSED_DIR = f\"{DATA_DIR}/processed\"\n",
    "\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Raw data: {RAW_DIR}\")\n",
    "print(f\"Processed data: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f0a0c8",
   "metadata": {
    "papermill": {
     "duration": 0.005607,
     "end_time": "2025-12-05T09:29:08.401594",
     "exception": false,
     "start_time": "2025-12-05T09:29:08.395987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## 1. Load Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08988fc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:08.415787Z",
     "iopub.status.busy": "2025-12-05T09:29:08.415439Z",
     "iopub.status.idle": "2025-12-05T09:29:08.420019Z",
     "shell.execute_reply": "2025-12-05T09:29:08.419116Z"
    },
    "papermill": {
     "duration": 0.012335,
     "end_time": "2025-12-05T09:29:08.420834",
     "exception": false,
     "start_time": "2025-12-05T09:29:08.408499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available raw data:\n",
      "  [DIR] kormedmcqa\n",
      "  [DIR] medical_reasoning_kormedmcqa\n",
      "  [FILE] korean_corpus_for_tokenizer.txt (221.6 MB)\n"
     ]
    }
   ],
   "source": [
    "# Check what data is available\n",
    "print(\"Available raw data:\")\n",
    "for item in os.listdir(RAW_DIR):\n",
    "    path = os.path.join(RAW_DIR, item)\n",
    "    if os.path.isdir(path):\n",
    "        print(f\"  [DIR] {item}\")\n",
    "    else:\n",
    "        size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "        print(f\"  [FILE] {item} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71647344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:08.434070Z",
     "iopub.status.busy": "2025-12-05T09:29:08.433898Z",
     "iopub.status.idle": "2025-12-05T09:29:08.447753Z",
     "shell.execute_reply": "2025-12-05T09:29:08.446764Z"
    },
    "papermill": {
     "duration": 0.021688,
     "end_time": "2025-12-05T09:29:08.448554",
     "exception": false,
     "start_time": "2025-12-05T09:29:08.426866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded KorMedMCQA: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['subject', 'year', 'period', 'q_number', 'question', 'A', 'B', 'C', 'D', 'E', 'answer', 'cot', 'exam_type'],\n",
      "        num_rows: 1890\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['subject', 'year', 'period', 'q_number', 'question', 'A', 'B', 'C', 'D', 'E', 'answer', 'cot', 'exam_type'],\n",
      "        num_rows: 604\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load KorMedMCQA\n",
    "kormedmcqa_path = f\"{RAW_DIR}/kormedmcqa\"\n",
    "if os.path.exists(kormedmcqa_path):\n",
    "    kormedmcqa = load_from_disk(kormedmcqa_path)\n",
    "    print(f\"Loaded KorMedMCQA: {kormedmcqa}\")\n",
    "else:\n",
    "    print(\"KorMedMCQA not found, run 02_collect_korean_medical.ipynb first\")\n",
    "    kormedmcqa = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8810aacf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:08.462128Z",
     "iopub.status.busy": "2025-12-05T09:29:08.461947Z",
     "iopub.status.idle": "2025-12-05T09:29:08.466458Z",
     "shell.execute_reply": "2025-12-05T09:29:08.465448Z"
    },
    "papermill": {
     "duration": 0.012646,
     "end_time": "2025-12-05T09:29:08.467344",
     "exception": false,
     "start_time": "2025-12-05T09:29:08.454698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical Wikipedia not found\n"
     ]
    }
   ],
   "source": [
    "# Load Medical Wikipedia\n",
    "wiki_medical_path = f\"{RAW_DIR}/wiki_medical_ko\"\n",
    "if os.path.exists(wiki_medical_path):\n",
    "    wiki_medical = load_from_disk(wiki_medical_path)\n",
    "    print(f\"Loaded Medical Wikipedia: {len(wiki_medical)} articles\")\n",
    "else:\n",
    "    print(\"Medical Wikipedia not found\")\n",
    "    wiki_medical = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a902c3d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:08.481457Z",
     "iopub.status.busy": "2025-12-05T09:29:08.481168Z",
     "iopub.status.idle": "2025-12-05T09:29:08.494329Z",
     "shell.execute_reply": "2025-12-05T09:29:08.493263Z"
    },
    "papermill": {
     "duration": 0.02122,
     "end_time": "2025-12-05T09:29:08.495205",
     "exception": false,
     "start_time": "2025-12-05T09:29:08.473985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Medical Reasoning: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['subject', 'year', 'period', 'q_number', 'question', 'A', 'B', 'C', 'D', 'E', 'answer', 'thinking', 'response', '__index_level_0__'],\n",
      "        num_rows: 8751\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load Medical Reasoning (optional)\n",
    "med_reasoning_path = f\"{RAW_DIR}/medical_reasoning_kormedmcqa\"\n",
    "if os.path.exists(med_reasoning_path):\n",
    "    med_reasoning = load_from_disk(med_reasoning_path)\n",
    "    print(f\"Loaded Medical Reasoning: {med_reasoning}\")\n",
    "else:\n",
    "    print(\"Medical Reasoning not found\")\n",
    "    med_reasoning = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c261b4",
   "metadata": {
    "papermill": {
     "duration": 0.003717,
     "end_time": "2025-12-05T09:29:08.502736",
     "exception": false,
     "start_time": "2025-12-05T09:29:08.499019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## 2. Format for Language Modeling (Stage 1-5)\n",
    "\n",
    "Plain text format for embedding training stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a771428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:08.511856Z",
     "iopub.status.busy": "2025-12-05T09:29:08.511560Z",
     "iopub.status.idle": "2025-12-05T09:29:08.517176Z",
     "shell.execute_reply": "2025-12-05T09:29:08.516167Z"
    },
    "papermill": {
     "duration": 0.011446,
     "end_time": "2025-12-05T09:29:08.517890",
     "exception": false,
     "start_time": "2025-12-05T09:29:08.506444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove very short lines\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def format_for_lm(examples):\n",
    "    \"\"\"Format examples for language modeling\"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    for text in examples[\"text\"]:\n",
    "        cleaned = clean_text(text)\n",
    "        if len(cleaned) > 100:  # Minimum length\n",
    "            texts.append(cleaned)\n",
    "    \n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dafbfc75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:08.526773Z",
     "iopub.status.busy": "2025-12-05T09:29:08.526495Z",
     "iopub.status.idle": "2025-12-05T09:29:08.531392Z",
     "shell.execute_reply": "2025-12-05T09:29:08.530350Z"
    },
    "papermill": {
     "duration": 0.010397,
     "end_time": "2025-12-05T09:29:08.532142",
     "exception": false,
     "start_time": "2025-12-05T09:29:08.521745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process Medical Wikipedia for LM\n",
    "if wiki_medical:\n",
    "    lm_texts = []\n",
    "    \n",
    "    for article in tqdm(wiki_medical, desc=\"Processing Wikipedia\"):\n",
    "        text = clean_text(article[\"text\"])\n",
    "        if len(text) > 100:\n",
    "            lm_texts.append({\"text\": text})\n",
    "    \n",
    "    wiki_lm = Dataset.from_list(lm_texts)\n",
    "    print(f\"Medical Wikipedia for LM: {len(wiki_lm)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d83ee33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:08.540909Z",
     "iopub.status.busy": "2025-12-05T09:29:08.540635Z",
     "iopub.status.idle": "2025-12-05T09:29:16.690062Z",
     "shell.execute_reply": "2025-12-05T09:29:16.689018Z"
    },
    "papermill": {
     "duration": 8.155233,
     "end_time": "2025-12-05T09:29:16.690926",
     "exception": false,
     "start_time": "2025-12-05T09:29:08.535693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer corpus for LM training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 285it [00:00, 2839.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 693it [00:00, 3567.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 1050it [00:00, 3500.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 1491it [00:00, 3855.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 2310it [00:00, 5405.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 2986it [00:00, 5861.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 3574it [00:00, 4988.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 4095it [00:00, 4903.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 4600it [00:00, 4729.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 5083it [00:01, 4472.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 5542it [00:01, 4503.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 6023it [00:01, 4586.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 6619it [00:01, 4977.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 8412it [00:01, 8727.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 9332it [00:01, 8857.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 10232it [00:01, 8008.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 11058it [00:01, 6962.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 11793it [00:02, 6121.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 12445it [00:02, 5839.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 13118it [00:02, 6055.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 13747it [00:02, 5989.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 14436it [00:02, 6226.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 15073it [00:02, 6050.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 15915it [00:02, 6700.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 16687it [00:02, 6986.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 17408it [00:02, 7048.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 18251it [00:03, 7445.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 19296it [00:03, 8318.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 20247it [00:03, 8664.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 21119it [00:03, 7909.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 21926it [00:03, 7706.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 22766it [00:03, 7899.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 23566it [00:03, 7384.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 24317it [00:03, 7267.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 25129it [00:03, 7502.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 25928it [00:03, 7639.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 26715it [00:04, 7705.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 27490it [00:04, 7660.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 28260it [00:04, 7663.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 29029it [00:04, 7052.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 30277it [00:04, 8562.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 31152it [00:04, 8443.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 32088it [00:04, 8694.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 33037it [00:04, 8923.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 33967it [00:04, 9030.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 34876it [00:05, 8732.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 35756it [00:05, 8544.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 36615it [00:05, 7766.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 37407it [00:05, 7641.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 38308it [00:05, 8016.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 39120it [00:05, 7806.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 39968it [00:05, 7995.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 40787it [00:05, 8046.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 41748it [00:05, 8499.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 42692it [00:06, 8773.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading corpus: 43566it [00:06, 7155.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Korean corpus for LM: 43365 documents\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer corpus and create LM dataset\n",
    "tokenizer_corpus_path = f\"{RAW_DIR}/korean_corpus_for_tokenizer.txt\"\n",
    "\n",
    "if os.path.exists(tokenizer_corpus_path):\n",
    "    print(\"Loading tokenizer corpus for LM training...\")\n",
    "    \n",
    "    lm_general_texts = []\n",
    "    with open(tokenizer_corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(tqdm(f, desc=\"Reading corpus\")):\n",
    "            text = clean_text(line)\n",
    "            if len(text) > 100:\n",
    "                lm_general_texts.append({\"text\": text})\n",
    "            \n",
    "            # Limit to manageable size for LM training\n",
    "            if len(lm_general_texts) >= 1000000:  # 1M documents\n",
    "                break\n",
    "    \n",
    "    general_lm = Dataset.from_list(lm_general_texts)\n",
    "    print(f\"General Korean corpus for LM: {len(general_lm)} documents\")\n",
    "else:\n",
    "    print(\"Tokenizer corpus not found\")\n",
    "    general_lm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2467d7d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:16.714644Z",
     "iopub.status.busy": "2025-12-05T09:29:16.714467Z",
     "iopub.status.idle": "2025-12-05T09:29:16.733505Z",
     "shell.execute_reply": "2025-12-05T09:29:16.732547Z"
    },
    "papermill": {
     "duration": 0.032316,
     "end_time": "2025-12-05T09:29:16.734667",
     "exception": false,
     "start_time": "2025-12-05T09:29:16.702351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined LM dataset: 43365 documents\n",
      "Sample: 555(오백오십오)는 554보다 크고 556보다 작은 자연수이다. 수학 합성수로, 그 약수는 1, 3, 5, 15, 37, 111, 185, 555이다. 기타 미국에서는 영화, 드라마, 만화, 노래 등에서 가상의 전화번호를 사용할 경우 반드시 555로 시작하는 번호를 사용해야 한다는 규정이 있다. 5e02 555...\n"
     ]
    }
   ],
   "source": [
    "# Combine LM datasets\n",
    "lm_datasets = []\n",
    "\n",
    "if wiki_medical:\n",
    "    lm_datasets.append(wiki_lm)\n",
    "    \n",
    "if general_lm:\n",
    "    lm_datasets.append(general_lm)\n",
    "\n",
    "if lm_datasets:\n",
    "    combined_lm = concatenate_datasets(lm_datasets)\n",
    "    \n",
    "    # Shuffle\n",
    "    combined_lm = combined_lm.shuffle(seed=42)\n",
    "    \n",
    "    print(f\"\\nCombined LM dataset: {len(combined_lm)} documents\")\n",
    "    print(f\"Sample: {combined_lm[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd3dce",
   "metadata": {
    "papermill": {
     "duration": 0.010751,
     "end_time": "2025-12-05T09:29:16.756612",
     "exception": false,
     "start_time": "2025-12-05T09:29:16.745861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## 3. Format for Instruction Tuning\n",
    "\n",
    "ChatML format for medical QA instruction tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4561b84e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:16.776123Z",
     "iopub.status.busy": "2025-12-05T09:29:16.775945Z",
     "iopub.status.idle": "2025-12-05T09:29:16.782877Z",
     "shell.execute_reply": "2025-12-05T09:29:16.781911Z"
    },
    "papermill": {
     "duration": 0.016251,
     "end_time": "2025-12-05T09:29:16.783790",
     "exception": false,
     "start_time": "2025-12-05T09:29:16.767539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test example keys: dict_keys(['subject', 'year', 'period', 'q_number', 'question', 'A', 'B', 'C', 'D', 'E', 'answer', 'cot', 'exam_type'])\n",
      "\n",
      "Sample formatted instruction:\n",
      "<|im_start|>system\n",
      "당신은 한국어 의료 전문 AI 어시스턴트입니다. 정확하고 도움이 되는 의료 정보를 제공하세요. 의료 질문에 대해 전문적이고 이해하기 쉬운 답변을 제공합니다.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "항문압 측정 검사에서 항문 압력이 증가하는 경우는?\n",
      "\n",
      "1. 직장질루(rectovaginal fistula)\n",
      "2. 항문열창(anal fissure)\n",
      "3. 대변실금(fecal incontinence)\n",
      "4. 대변메막힘(fecal impaction)\n",
      "5. 직장탈출증(rectal prolapse)\n",
      "\n",
      "위 질문에 대한 정답을 선택하고 설명해주세요.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "정답은 2번입니다.\n",
      "\n",
      "항문열창(anal fissure)\n",
      "\n",
      "이 답이 정답인 이유는 해당 의학적 지식에 기반하여 가장 적절한 선택이기 때문입니다.\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Instruction template\n",
    "SYSTEM_PROMPT_KO = \"\"\"당신은 한국어 의료 전문 AI 어시스턴트입니다. 정확하고 도움이 되는 의료 정보를 제공하세요. 의료 질문에 대해 전문적이고 이해하기 쉬운 답변을 제공합니다.\"\"\"\n",
    "\n",
    "def format_kormedmcqa_for_instruction(example):\n",
    "    \"\"\"Format KorMedMCQA as instruction-following data\"\"\"\n",
    "    \n",
    "    question = example[\"question\"]\n",
    "    \n",
    "    # KorMedMCQA has choices in separate columns A, B, C, D, E\n",
    "    choices = []\n",
    "    for letter in ['A', 'B', 'C', 'D', 'E']:\n",
    "        if letter in example and example[letter]:\n",
    "            choices.append(example[letter])\n",
    "    \n",
    "    answer_idx = example[\"answer\"]  # 1-indexed in the dataset\n",
    "    \n",
    "    # Format choices\n",
    "    formatted_choices = \"\\n\".join([f\"{i+1}. {c}\" for i, c in enumerate(choices)])\n",
    "    \n",
    "    # Create user message\n",
    "    user_message = f\"{question}\\n\\n{formatted_choices}\\n\\n위 질문에 대한 정답을 선택하고 설명해주세요.\"\n",
    "    \n",
    "    # Create assistant response\n",
    "    correct_answer = choices[answer_idx - 1] if answer_idx > 0 and answer_idx <= len(choices) else choices[0]\n",
    "    assistant_message = f\"정답은 {answer_idx}번입니다.\\n\\n{correct_answer}\\n\\n이 답이 정답인 이유는 해당 의학적 지식에 기반하여 가장 적절한 선택이기 때문입니다.\"\n",
    "    \n",
    "    # Add Chain-of-Thought if available\n",
    "    if example.get(\"cot\"):\n",
    "        assistant_message = f\"{example['cot']}\\n\\n따라서 정답은 {answer_idx}번 '{correct_answer}'입니다.\"\n",
    "    \n",
    "    # Format as ChatML\n",
    "    text = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT_KO}\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{user_message}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{assistant_message}\n",
    "<|im_end|>\"\"\"\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "# Test\n",
    "if kormedmcqa:\n",
    "    test_example = kormedmcqa[\"train\"][0]\n",
    "    print(\"Test example keys:\", test_example.keys())\n",
    "    formatted = format_kormedmcqa_for_instruction(test_example)\n",
    "    print(\"\\nSample formatted instruction:\")\n",
    "    print(formatted[\"text\"][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2446a96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:16.798690Z",
     "iopub.status.busy": "2025-12-05T09:29:16.798369Z",
     "iopub.status.idle": "2025-12-05T09:29:17.099186Z",
     "shell.execute_reply": "2025-12-05T09:29:17.098274Z"
    },
    "papermill": {
     "duration": 0.310078,
     "end_time": "2025-12-05T09:29:17.100662",
     "exception": false,
     "start_time": "2025-12-05T09:29:16.790584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting train:   0%|          | 0/1890 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting train:  45%|████▍     | 849/1890 [00:00<00:00, 8489.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting train:  98%|█████████▊| 1849/1890 [00:00<00:00, 9374.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting train: 100%|██████████| 1890/1890 [00:00<00:00, 9202.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting test:   0%|          | 0/604 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting test: 100%|██████████| 604/604 [00:00<00:00, 9960.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instruction dataset: 2494 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Format all KorMedMCQA data for instruction tuning\n",
    "if kormedmcqa:\n",
    "    instruction_data = []\n",
    "    \n",
    "    for split in kormedmcqa.keys():\n",
    "        for example in tqdm(kormedmcqa[split], desc=f\"Formatting {split}\"):\n",
    "            formatted = format_kormedmcqa_for_instruction(example)\n",
    "            instruction_data.append(formatted)\n",
    "    \n",
    "    instruction_dataset = Dataset.from_list(instruction_data)\n",
    "    print(f\"\\nInstruction dataset: {len(instruction_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea2af72b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:17.126577Z",
     "iopub.status.busy": "2025-12-05T09:29:17.126389Z",
     "iopub.status.idle": "2025-12-05T09:29:18.110835Z",
     "shell.execute_reply": "2025-12-05T09:29:18.109891Z"
    },
    "papermill": {
     "duration": 0.998529,
     "end_time": "2025-12-05T09:29:18.111708",
     "exception": false,
     "start_time": "2025-12-05T09:29:17.113179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Chain-of-Thought examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting CoT:   0%|          | 0/8751 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting CoT:  10%|█         | 907/8751 [00:00<00:00, 9062.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting CoT:  21%|██        | 1814/8751 [00:00<00:00, 9065.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting CoT:  31%|███       | 2721/8751 [00:00<00:00, 9002.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting CoT:  41%|████▏     | 3622/8751 [00:00<00:00, 8965.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting CoT:  52%|█████▏    | 4519/8751 [00:00<00:00, 8952.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting CoT:  62%|██████▏   | 5422/8751 [00:00<00:00, 8976.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting CoT:  72%|███████▏  | 6320/8751 [00:00<00:00, 8976.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting CoT:  82%|████████▏ | 7218/8751 [00:00<00:00, 8904.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting CoT:  93%|█████████▎| 8109/8751 [00:00<00:00, 8902.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting CoT: 100%|██████████| 8751/8751 [00:00<00:00, 8956.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add Chain-of-Thought examples if available\n",
    "if med_reasoning:\n",
    "    print(\"Adding Chain-of-Thought examples...\")\n",
    "    \n",
    "    cot_data = []\n",
    "    split_name = list(med_reasoning.keys())[0]\n",
    "    \n",
    "    for example in tqdm(med_reasoning[split_name], desc=\"Formatting CoT\"):\n",
    "        # Check what fields are available\n",
    "        if \"reasoning\" in example:\n",
    "            question = example.get(\"question\", \"\")\n",
    "            reasoning = example.get(\"reasoning\", \"\")\n",
    "            answer = example.get(\"answer\", \"\")\n",
    "            \n",
    "            user_message = f\"{question}\\n\\n단계별로 생각하며 답변해주세요.\"\n",
    "            assistant_message = f\"{reasoning}\\n\\n따라서 정답은 {answer}입니다.\"\n",
    "            \n",
    "            text = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT_KO}\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{user_message}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{assistant_message}\n",
    "<|im_end|>\"\"\"\n",
    "            \n",
    "            cot_data.append({\"text\": text})\n",
    "    \n",
    "    if cot_data:\n",
    "        cot_dataset = Dataset.from_list(cot_data)\n",
    "        print(f\"CoT dataset: {len(cot_dataset)} examples\")\n",
    "        \n",
    "        # Combine with instruction dataset\n",
    "        instruction_dataset = concatenate_datasets([instruction_dataset, cot_dataset])\n",
    "        print(f\"Combined instruction dataset: {len(instruction_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b4912",
   "metadata": {
    "papermill": {
     "duration": 0.01271,
     "end_time": "2025-12-05T09:29:18.137496",
     "exception": false,
     "start_time": "2025-12-05T09:29:18.124786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## 4. Create Mixed Dataset (Korean 90% + English 10%)\n",
    "\n",
    "For Stage 6 training to prevent catastrophic forgetting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8d9bd53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:18.163604Z",
     "iopub.status.busy": "2025-12-05T09:29:18.163427Z",
     "iopub.status.idle": "2025-12-05T09:29:18.167736Z",
     "shell.execute_reply": "2025-12-05T09:29:18.166862Z"
    },
    "papermill": {
     "duration": 0.018041,
     "end_time": "2025-12-05T09:29:18.168382",
     "exception": false,
     "start_time": "2025-12-05T09:29:18.150341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean data: 43365\n",
      "Target English data: 4818\n",
      "\n",
      "Note: Load actual English medical data for production use.\n"
     ]
    }
   ],
   "source": [
    "# Note: In practice, you would load English medical data here\n",
    "# For now, we'll create a placeholder structure\n",
    "\n",
    "def create_mixed_dataset(korean_data, english_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Create mixed Korean/English dataset.\n",
    "    \n",
    "    In practice, load English medical data from:\n",
    "    - MedQA\n",
    "    - PubMed abstracts\n",
    "    - Medical textbooks\n",
    "    \"\"\"\n",
    "    \n",
    "    korean_size = len(korean_data)\n",
    "    target_english_size = int(korean_size * english_ratio / (1 - english_ratio))\n",
    "    \n",
    "    print(f\"Korean data: {korean_size}\")\n",
    "    print(f\"Target English data: {target_english_size}\")\n",
    "    print(\"\\nNote: Load actual English medical data for production use.\")\n",
    "    \n",
    "    # For now, return Korean-only\n",
    "    return korean_data\n",
    "\n",
    "if combined_lm:\n",
    "    mixed_lm = create_mixed_dataset(combined_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c1b581",
   "metadata": {
    "papermill": {
     "duration": 0.007133,
     "end_time": "2025-12-05T09:29:18.182890",
     "exception": false,
     "start_time": "2025-12-05T09:29:18.175757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## 5. Create Train/Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c5eb7d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:18.237677Z",
     "iopub.status.busy": "2025-12-05T09:29:18.237495Z",
     "iopub.status.idle": "2025-12-05T09:29:18.241052Z",
     "shell.execute_reply": "2025-12-05T09:29:18.240131Z"
    },
    "papermill": {
     "duration": 0.052007,
     "end_time": "2025-12-05T09:29:18.241787",
     "exception": false,
     "start_time": "2025-12-05T09:29:18.189780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_splits(dataset, test_size=0.1, seed=42):\n",
    "    \"\"\"Split dataset into train and validation\"\"\"\n",
    "    \n",
    "    split_data = dataset.train_test_split(test_size=test_size, seed=seed)\n",
    "    \n",
    "    return DatasetDict({\n",
    "        \"train\": split_data[\"train\"],\n",
    "        \"validation\": split_data[\"test\"],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b315b39f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:18.258387Z",
     "iopub.status.busy": "2025-12-05T09:29:18.258217Z",
     "iopub.status.idle": "2025-12-05T09:29:18.274107Z",
     "shell.execute_reply": "2025-12-05T09:29:18.273130Z"
    },
    "papermill": {
     "duration": 0.02476,
     "end_time": "2025-12-05T09:29:18.274808",
     "exception": false,
     "start_time": "2025-12-05T09:29:18.250048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM dataset splits:\n",
      "  Train: 41196\n",
      "  Validation: 2169\n"
     ]
    }
   ],
   "source": [
    "# Create splits for LM dataset\n",
    "if combined_lm:\n",
    "    lm_splits = create_splits(combined_lm, test_size=0.05)\n",
    "    print(f\"LM dataset splits:\")\n",
    "    print(f\"  Train: {len(lm_splits['train'])}\")\n",
    "    print(f\"  Validation: {len(lm_splits['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7c9aa39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:18.291109Z",
     "iopub.status.busy": "2025-12-05T09:29:18.290936Z",
     "iopub.status.idle": "2025-12-05T09:29:18.297988Z",
     "shell.execute_reply": "2025-12-05T09:29:18.297080Z"
    },
    "papermill": {
     "duration": 0.016257,
     "end_time": "2025-12-05T09:29:18.298701",
     "exception": false,
     "start_time": "2025-12-05T09:29:18.282444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction dataset splits:\n",
      "  Train: 2244\n",
      "  Validation: 250\n"
     ]
    }
   ],
   "source": [
    "# Create splits for instruction dataset\n",
    "if instruction_dataset:\n",
    "    instruction_splits = create_splits(instruction_dataset, test_size=0.1)\n",
    "    print(f\"Instruction dataset splits:\")\n",
    "    print(f\"  Train: {len(instruction_splits['train'])}\")\n",
    "    print(f\"  Validation: {len(instruction_splits['validation'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ca40c1",
   "metadata": {
    "papermill": {
     "duration": 0.007416,
     "end_time": "2025-12-05T09:29:18.313552",
     "exception": false,
     "start_time": "2025-12-05T09:29:18.306136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## 6. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00877e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:18.329045Z",
     "iopub.status.busy": "2025-12-05T09:29:18.328873Z",
     "iopub.status.idle": "2025-12-05T09:29:18.725925Z",
     "shell.execute_reply": "2025-12-05T09:29:18.724887Z"
    },
    "papermill": {
     "duration": 0.406211,
     "end_time": "2025-12-05T09:29:18.726757",
     "exception": false,
     "start_time": "2025-12-05T09:29:18.320546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (0/1 shards):   0%|          | 0/41196 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (0/1 shards):  29%|██▉       | 12000/41196 [00:00<00:00, 111155.48 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (0/1 shards):  58%|█████▊    | 24000/41196 [00:00<00:00, 113640.99 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (0/1 shards):  87%|████████▋ | 36000/41196 [00:00<00:00, 114154.18 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (1/1 shards): 100%|██████████| 41196/41196 [00:00<00:00, 114154.18 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (1/1 shards): 100%|██████████| 41196/41196 [00:00<00:00, 113110.23 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (0/1 shards):   0%|          | 0/2169 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (1/1 shards): 100%|██████████| 2169/2169 [00:00<00:00, 100802.72 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (1/1 shards): 100%|██████████| 2169/2169 [00:00<00:00, 97244.80 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LM dataset to ../data/processed/korean_medical_lm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Save LM dataset\n",
    "if lm_splits:\n",
    "    lm_path = f\"{PROCESSED_DIR}/korean_medical_lm\"\n",
    "    lm_splits.save_to_disk(lm_path)\n",
    "    print(f\"Saved LM dataset to {lm_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1db0338",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:18.757135Z",
     "iopub.status.busy": "2025-12-05T09:29:18.756957Z",
     "iopub.status.idle": "2025-12-05T09:29:18.785661Z",
     "shell.execute_reply": "2025-12-05T09:29:18.784770Z"
    },
    "papermill": {
     "duration": 0.045457,
     "end_time": "2025-12-05T09:29:18.786914",
     "exception": false,
     "start_time": "2025-12-05T09:29:18.741457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (0/1 shards):   0%|          | 0/2244 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (1/1 shards): 100%|██████████| 2244/2244 [00:00<00:00, 168764.89 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (1/1 shards): 100%|██████████| 2244/2244 [00:00<00:00, 159369.06 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (0/1 shards):   0%|          | 0/250 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (1/1 shards): 100%|██████████| 250/250 [00:00<00:00, 75021.54 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (1/1 shards): 100%|██████████| 250/250 [00:00<00:00, 59792.21 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved instruction dataset to ../data/processed/korean_medical_instruction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Save instruction dataset\n",
    "if instruction_splits:\n",
    "    instruction_path = f\"{PROCESSED_DIR}/korean_medical_instruction\"\n",
    "    instruction_splits.save_to_disk(instruction_path)\n",
    "    print(f\"Saved instruction dataset to {instruction_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e55c794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:18.817386Z",
     "iopub.status.busy": "2025-12-05T09:29:18.817200Z",
     "iopub.status.idle": "2025-12-05T09:29:18.831234Z",
     "shell.execute_reply": "2025-12-05T09:29:18.830336Z"
    },
    "papermill": {
     "duration": 0.029976,
     "end_time": "2025-12-05T09:29:18.832213",
     "exception": false,
     "start_time": "2025-12-05T09:29:18.802237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (0/1 shards):   0%|          | 0/604 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (1/1 shards): 100%|██████████| 604/604 [00:00<00:00, 102681.57 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (1/1 shards): 100%|██████████| 604/604 [00:00<00:00, 89870.50 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation dataset to ../data/processed/kormedmcqa_eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Save evaluation dataset (KorMedMCQA test set)\n",
    "if kormedmcqa and \"test\" in kormedmcqa:\n",
    "    eval_path = f\"{PROCESSED_DIR}/kormedmcqa_eval\"\n",
    "    kormedmcqa[\"test\"].save_to_disk(eval_path)\n",
    "    print(f\"Saved evaluation dataset to {eval_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4569d948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:18.851333Z",
     "iopub.status.busy": "2025-12-05T09:29:18.851155Z",
     "iopub.status.idle": "2025-12-05T09:29:18.857514Z",
     "shell.execute_reply": "2025-12-05T09:29:18.856599Z"
    },
    "papermill": {
     "duration": 0.01656,
     "end_time": "2025-12-05T09:29:18.858194",
     "exception": false,
     "start_time": "2025-12-05T09:29:18.841634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Data Processing Summary\n",
      "============================================================\n",
      "{\n",
      "  \"datasets\": {\n",
      "    \"korean_medical_lm\": {\n",
      "      \"path\": \"../data/processed/korean_medical_lm\",\n",
      "      \"train_size\": 41196,\n",
      "      \"validation_size\": 2169,\n",
      "      \"use\": \"Stage 1-5 (Embedding training)\"\n",
      "    },\n",
      "    \"korean_medical_instruction\": {\n",
      "      \"path\": \"../data/processed/korean_medical_instruction\",\n",
      "      \"train_size\": 2244,\n",
      "      \"validation_size\": 250,\n",
      "      \"use\": \"Stage 6-7 (Instruction tuning)\"\n",
      "    },\n",
      "    \"kormedmcqa_eval\": {\n",
      "      \"path\": \"../data/processed/kormedmcqa_eval\",\n",
      "      \"size\": 604,\n",
      "      \"use\": \"Evaluation\"\n",
      "    }\n",
      "  },\n",
      "  \"processing_date\": \"2025-12-05 09:29:18.853930\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create processing summary\n",
    "import datetime\n",
    "\n",
    "summary = {\n",
    "    \"datasets\": {},\n",
    "    \"processing_date\": str(datetime.datetime.now()),\n",
    "}\n",
    "\n",
    "if 'lm_splits' in dir() and lm_splits:\n",
    "    summary[\"datasets\"][\"korean_medical_lm\"] = {\n",
    "        \"path\": f\"{PROCESSED_DIR}/korean_medical_lm\",\n",
    "        \"train_size\": len(lm_splits[\"train\"]),\n",
    "        \"validation_size\": len(lm_splits[\"validation\"]),\n",
    "        \"use\": \"Stage 1-5 (Embedding training)\",\n",
    "    }\n",
    "\n",
    "if 'instruction_splits' in dir() and instruction_splits:\n",
    "    summary[\"datasets\"][\"korean_medical_instruction\"] = {\n",
    "        \"path\": f\"{PROCESSED_DIR}/korean_medical_instruction\",\n",
    "        \"train_size\": len(instruction_splits[\"train\"]),\n",
    "        \"validation_size\": len(instruction_splits[\"validation\"]),\n",
    "        \"use\": \"Stage 6-7 (Instruction tuning)\",\n",
    "    }\n",
    "\n",
    "if kormedmcqa and \"test\" in kormedmcqa:\n",
    "    summary[\"datasets\"][\"kormedmcqa_eval\"] = {\n",
    "        \"path\": f\"{PROCESSED_DIR}/kormedmcqa_eval\",\n",
    "        \"size\": len(kormedmcqa[\"test\"]),\n",
    "        \"use\": \"Evaluation\",\n",
    "    }\n",
    "\n",
    "# Save summary\n",
    "summary_path = f\"{PROCESSED_DIR}/processing_summary.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Data Processing Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(summary, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae1db390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T09:29:18.877903Z",
     "iopub.status.busy": "2025-12-05T09:29:18.877733Z",
     "iopub.status.idle": "2025-12-05T09:29:18.881751Z",
     "shell.execute_reply": "2025-12-05T09:29:18.880828Z"
    },
    "papermill": {
     "duration": 0.015113,
     "end_time": "2025-12-05T09:29:18.882370",
     "exception": false,
     "start_time": "2025-12-05T09:29:18.867257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Phase 0: Data Preparation Complete!\n",
      "============================================================\n",
      "\n",
      "Processed data saved to: ../data/processed\n",
      "\n",
      "Next steps:\n",
      "  1. Move to Phase 1: Tokenizer Training\n",
      "  2. Run phase1_tokenizer/01_train_korean_tokenizer.ipynb\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Phase 0: Data Preparation Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nProcessed data saved to: {PROCESSED_DIR}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Move to Phase 1: Tokenizer Training\")\n",
    "print(\"  2. Run phase1_tokenizer/01_train_korean_tokenizer.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.412966,
   "end_time": "2025-12-05T09:29:19.809398",
   "environment_variables": {},
   "exception": null,
   "input_path": "04_preprocess_data.ipynb",
   "output_path": "04_preprocess_data_output.ipynb",
   "parameters": {},
   "start_time": "2025-12-05T09:29:05.396432",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}