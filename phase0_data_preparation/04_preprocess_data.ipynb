{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 0.4: Preprocess and Format Training Data\n",
    "\n",
    "Prepare and format all collected data for training stages.\n",
    "\n",
    "## Contents\n",
    "1. Load Collected Data\n",
    "2. Format for Language Modeling (Stage 1-5)\n",
    "3. Format for Instruction Tuning (Stage 6-7)\n",
    "4. Create Mixed Dataset (Korean + English)\n",
    "5. Create Evaluation Splits\n",
    "6. Save All Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from datasets import load_from_disk, Dataset, DatasetDict, concatenate_datasets\n",
    "import random\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"../data\"\n",
    "RAW_DIR = f\"{DATA_DIR}/raw\"\n",
    "PROCESSED_DIR = f\"{DATA_DIR}/processed\"\n",
    "\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Raw data: {RAW_DIR}\")\n",
    "print(f\"Processed data: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what data is available\n",
    "print(\"Available raw data:\")\n",
    "for item in os.listdir(RAW_DIR):\n",
    "    path = os.path.join(RAW_DIR, item)\n",
    "    if os.path.isdir(path):\n",
    "        print(f\"  [DIR] {item}\")\n",
    "    else:\n",
    "        size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "        print(f\"  [FILE] {item} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KorMedMCQA\n",
    "kormedmcqa_path = f\"{RAW_DIR}/kormedmcqa\"\n",
    "if os.path.exists(kormedmcqa_path):\n",
    "    kormedmcqa = load_from_disk(kormedmcqa_path)\n",
    "    print(f\"Loaded KorMedMCQA: {kormedmcqa}\")\n",
    "else:\n",
    "    print(\"KorMedMCQA not found, run 02_collect_korean_medical.ipynb first\")\n",
    "    kormedmcqa = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Medical Wikipedia\n",
    "wiki_medical_path = f\"{RAW_DIR}/wiki_medical_ko\"\n",
    "if os.path.exists(wiki_medical_path):\n",
    "    wiki_medical = load_from_disk(wiki_medical_path)\n",
    "    print(f\"Loaded Medical Wikipedia: {len(wiki_medical)} articles\")\n",
    "else:\n",
    "    print(\"Medical Wikipedia not found\")\n",
    "    wiki_medical = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Medical Reasoning (optional)\n",
    "med_reasoning_path = f\"{RAW_DIR}/medical_reasoning_kormedmcqa\"\n",
    "if os.path.exists(med_reasoning_path):\n",
    "    med_reasoning = load_from_disk(med_reasoning_path)\n",
    "    print(f\"Loaded Medical Reasoning: {med_reasoning}\")\n",
    "else:\n",
    "    print(\"Medical Reasoning not found\")\n",
    "    med_reasoning = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Format for Language Modeling (Stage 1-5)\n",
    "\n",
    "Plain text format for embedding training stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove very short lines\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def format_for_lm(examples):\n",
    "    \"\"\"Format examples for language modeling\"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    for text in examples[\"text\"]:\n",
    "        cleaned = clean_text(text)\n",
    "        if len(cleaned) > 100:  # Minimum length\n",
    "            texts.append(cleaned)\n",
    "    \n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Medical Wikipedia for LM\n",
    "if wiki_medical:\n",
    "    lm_texts = []\n",
    "    \n",
    "    for article in tqdm(wiki_medical, desc=\"Processing Wikipedia\"):\n",
    "        text = clean_text(article[\"text\"])\n",
    "        if len(text) > 100:\n",
    "            lm_texts.append({\"text\": text})\n",
    "    \n",
    "    wiki_lm = Dataset.from_list(lm_texts)\n",
    "    print(f\"Medical Wikipedia for LM: {len(wiki_lm)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer corpus and create LM dataset\n",
    "tokenizer_corpus_path = f\"{RAW_DIR}/korean_corpus_for_tokenizer.txt\"\n",
    "\n",
    "if os.path.exists(tokenizer_corpus_path):\n",
    "    print(\"Loading tokenizer corpus for LM training...\")\n",
    "    \n",
    "    lm_general_texts = []\n",
    "    with open(tokenizer_corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(tqdm(f, desc=\"Reading corpus\")):\n",
    "            text = clean_text(line)\n",
    "            if len(text) > 100:\n",
    "                lm_general_texts.append({\"text\": text})\n",
    "            \n",
    "            # Limit to manageable size for LM training\n",
    "            if len(lm_general_texts) >= 1000000:  # 1M documents\n",
    "                break\n",
    "    \n",
    "    general_lm = Dataset.from_list(lm_general_texts)\n",
    "    print(f\"General Korean corpus for LM: {len(general_lm)} documents\")\n",
    "else:\n",
    "    print(\"Tokenizer corpus not found\")\n",
    "    general_lm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine LM datasets\n",
    "lm_datasets = []\n",
    "\n",
    "if wiki_medical:\n",
    "    lm_datasets.append(wiki_lm)\n",
    "    \n",
    "if general_lm:\n",
    "    lm_datasets.append(general_lm)\n",
    "\n",
    "if lm_datasets:\n",
    "    combined_lm = concatenate_datasets(lm_datasets)\n",
    "    \n",
    "    # Shuffle\n",
    "    combined_lm = combined_lm.shuffle(seed=42)\n",
    "    \n",
    "    print(f\"\\nCombined LM dataset: {len(combined_lm)} documents\")\n",
    "    print(f\"Sample: {combined_lm[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Format for Instruction Tuning\n",
    "\n",
    "ChatML format for medical QA instruction tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Instruction template\nSYSTEM_PROMPT_KO = \"\"\"당신은 한국어 의료 전문 AI 어시스턴트입니다. 정확하고 도움이 되는 의료 정보를 제공하세요. 의료 질문에 대해 전문적이고 이해하기 쉬운 답변을 제공합니다.\"\"\"\n\ndef format_kormedmcqa_for_instruction(example):\n    \"\"\"Format KorMedMCQA as instruction-following data\"\"\"\n    \n    question = example[\"question\"]\n    \n    # KorMedMCQA has choices in separate columns A, B, C, D, E\n    choices = []\n    for letter in ['A', 'B', 'C', 'D', 'E']:\n        if letter in example and example[letter]:\n            choices.append(example[letter])\n    \n    answer_idx = example[\"answer\"]  # 1-indexed in the dataset\n    \n    # Format choices\n    formatted_choices = \"\\n\".join([f\"{i+1}. {c}\" for i, c in enumerate(choices)])\n    \n    # Create user message\n    user_message = f\"{question}\\n\\n{formatted_choices}\\n\\n위 질문에 대한 정답을 선택하고 설명해주세요.\"\n    \n    # Create assistant response\n    correct_answer = choices[answer_idx - 1] if answer_idx > 0 and answer_idx <= len(choices) else choices[0]\n    assistant_message = f\"정답은 {answer_idx}번입니다.\\n\\n{correct_answer}\\n\\n이 답이 정답인 이유는 해당 의학적 지식에 기반하여 가장 적절한 선택이기 때문입니다.\"\n    \n    # Add Chain-of-Thought if available\n    if example.get(\"cot\"):\n        assistant_message = f\"{example['cot']}\\n\\n따라서 정답은 {answer_idx}번 '{correct_answer}'입니다.\"\n    \n    # Format as ChatML\n    text = f\"\"\"<|im_start|>system\n{SYSTEM_PROMPT_KO}\n<|im_end|>\n<|im_start|>user\n{user_message}\n<|im_end|>\n<|im_start|>assistant\n{assistant_message}\n<|im_end|>\"\"\"\n    \n    return {\"text\": text}\n\n# Test\nif kormedmcqa:\n    test_example = kormedmcqa[\"train\"][0]\n    print(\"Test example keys:\", test_example.keys())\n    formatted = format_kormedmcqa_for_instruction(test_example)\n    print(\"\\nSample formatted instruction:\")\n    print(formatted[\"text\"][:1000])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format all KorMedMCQA data for instruction tuning\n",
    "if kormedmcqa:\n",
    "    instruction_data = []\n",
    "    \n",
    "    for split in kormedmcqa.keys():\n",
    "        for example in tqdm(kormedmcqa[split], desc=f\"Formatting {split}\"):\n",
    "            formatted = format_kormedmcqa_for_instruction(example)\n",
    "            instruction_data.append(formatted)\n",
    "    \n",
    "    instruction_dataset = Dataset.from_list(instruction_data)\n",
    "    print(f\"\\nInstruction dataset: {len(instruction_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Chain-of-Thought examples if available\n",
    "if med_reasoning:\n",
    "    print(\"Adding Chain-of-Thought examples...\")\n",
    "    \n",
    "    cot_data = []\n",
    "    split_name = list(med_reasoning.keys())[0]\n",
    "    \n",
    "    for example in tqdm(med_reasoning[split_name], desc=\"Formatting CoT\"):\n",
    "        # Check what fields are available\n",
    "        if \"reasoning\" in example:\n",
    "            question = example.get(\"question\", \"\")\n",
    "            reasoning = example.get(\"reasoning\", \"\")\n",
    "            answer = example.get(\"answer\", \"\")\n",
    "            \n",
    "            user_message = f\"{question}\\n\\n단계별로 생각하며 답변해주세요.\"\n",
    "            assistant_message = f\"{reasoning}\\n\\n따라서 정답은 {answer}입니다.\"\n",
    "            \n",
    "            text = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT_KO}\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{user_message}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{assistant_message}\n",
    "<|im_end|>\"\"\"\n",
    "            \n",
    "            cot_data.append({\"text\": text})\n",
    "    \n",
    "    if cot_data:\n",
    "        cot_dataset = Dataset.from_list(cot_data)\n",
    "        print(f\"CoT dataset: {len(cot_dataset)} examples\")\n",
    "        \n",
    "        # Combine with instruction dataset\n",
    "        instruction_dataset = concatenate_datasets([instruction_dataset, cot_dataset])\n",
    "        print(f\"Combined instruction dataset: {len(instruction_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Create Mixed Dataset (Korean 90% + English 10%)\n",
    "\n",
    "For Stage 6 training to prevent catastrophic forgetting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: In practice, you would load English medical data here\n",
    "# For now, we'll create a placeholder structure\n",
    "\n",
    "def create_mixed_dataset(korean_data, english_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Create mixed Korean/English dataset.\n",
    "    \n",
    "    In practice, load English medical data from:\n",
    "    - MedQA\n",
    "    - PubMed abstracts\n",
    "    - Medical textbooks\n",
    "    \"\"\"\n",
    "    \n",
    "    korean_size = len(korean_data)\n",
    "    target_english_size = int(korean_size * english_ratio / (1 - english_ratio))\n",
    "    \n",
    "    print(f\"Korean data: {korean_size}\")\n",
    "    print(f\"Target English data: {target_english_size}\")\n",
    "    print(\"\\nNote: Load actual English medical data for production use.\")\n",
    "    \n",
    "    # For now, return Korean-only\n",
    "    return korean_data\n",
    "\n",
    "if combined_lm:\n",
    "    mixed_lm = create_mixed_dataset(combined_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Create Train/Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(dataset, test_size=0.1, seed=42):\n",
    "    \"\"\"Split dataset into train and validation\"\"\"\n",
    "    \n",
    "    split_data = dataset.train_test_split(test_size=test_size, seed=seed)\n",
    "    \n",
    "    return DatasetDict({\n",
    "        \"train\": split_data[\"train\"],\n",
    "        \"validation\": split_data[\"test\"],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits for LM dataset\n",
    "if combined_lm:\n",
    "    lm_splits = create_splits(combined_lm, test_size=0.05)\n",
    "    print(f\"LM dataset splits:\")\n",
    "    print(f\"  Train: {len(lm_splits['train'])}\")\n",
    "    print(f\"  Validation: {len(lm_splits['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits for instruction dataset\n",
    "if instruction_dataset:\n",
    "    instruction_splits = create_splits(instruction_dataset, test_size=0.1)\n",
    "    print(f\"Instruction dataset splits:\")\n",
    "    print(f\"  Train: {len(instruction_splits['train'])}\")\n",
    "    print(f\"  Validation: {len(instruction_splits['validation'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LM dataset\n",
    "if lm_splits:\n",
    "    lm_path = f\"{PROCESSED_DIR}/korean_medical_lm\"\n",
    "    lm_splits.save_to_disk(lm_path)\n",
    "    print(f\"Saved LM dataset to {lm_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save instruction dataset\n",
    "if instruction_splits:\n",
    "    instruction_path = f\"{PROCESSED_DIR}/korean_medical_instruction\"\n",
    "    instruction_splits.save_to_disk(instruction_path)\n",
    "    print(f\"Saved instruction dataset to {instruction_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation dataset (KorMedMCQA test set)\n",
    "if kormedmcqa and \"test\" in kormedmcqa:\n",
    "    eval_path = f\"{PROCESSED_DIR}/kormedmcqa_eval\"\n",
    "    kormedmcqa[\"test\"].save_to_disk(eval_path)\n",
    "    print(f\"Saved evaluation dataset to {eval_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create processing summary\nimport datetime\n\nsummary = {\n    \"datasets\": {},\n    \"processing_date\": str(datetime.datetime.now()),\n}\n\nif 'lm_splits' in dir() and lm_splits:\n    summary[\"datasets\"][\"korean_medical_lm\"] = {\n        \"path\": f\"{PROCESSED_DIR}/korean_medical_lm\",\n        \"train_size\": len(lm_splits[\"train\"]),\n        \"validation_size\": len(lm_splits[\"validation\"]),\n        \"use\": \"Stage 1-5 (Embedding training)\",\n    }\n\nif 'instruction_splits' in dir() and instruction_splits:\n    summary[\"datasets\"][\"korean_medical_instruction\"] = {\n        \"path\": f\"{PROCESSED_DIR}/korean_medical_instruction\",\n        \"train_size\": len(instruction_splits[\"train\"]),\n        \"validation_size\": len(instruction_splits[\"validation\"]),\n        \"use\": \"Stage 6-7 (Instruction tuning)\",\n    }\n\nif kormedmcqa and \"test\" in kormedmcqa:\n    summary[\"datasets\"][\"kormedmcqa_eval\"] = {\n        \"path\": f\"{PROCESSED_DIR}/kormedmcqa_eval\",\n        \"size\": len(kormedmcqa[\"test\"]),\n        \"use\": \"Evaluation\",\n    }\n\n# Save summary\nsummary_path = f\"{PROCESSED_DIR}/processing_summary.json\"\nwith open(summary_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(summary, f, ensure_ascii=False, indent=2)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Data Processing Summary\")\nprint(\"=\" * 60)\nprint(json.dumps(summary, indent=2, ensure_ascii=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Phase 0: Data Preparation Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nProcessed data saved to: {PROCESSED_DIR}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Move to Phase 1: Tokenizer Training\")\n",
    "print(\"  2. Run phase1_tokenizer/01_train_korean_tokenizer.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}