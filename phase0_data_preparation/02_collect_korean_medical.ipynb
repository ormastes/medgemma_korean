{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 0.2: Collect Korean Medical Datasets\n",
    "\n",
    "Download and prepare Korean medical datasets for training.\n",
    "\n",
    "## Contents\n",
    "1. Setup and GPU Configuration\n",
    "2. Download KorMedMCQA\n",
    "3. Download Medical Reasoning Dataset\n",
    "4. Download KorMedLawQA\n",
    "5. Filter Medical Content from Wikipedia\n",
    "6. Save All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# GPU setup\n",
    "from config.gpu_utils import setup_gpu, print_memory_usage\n",
    "device = setup_gpu()\n",
    "\n",
    "# Imports\n",
    "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
    "from huggingface_hub import HfApi\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Create data directories\n",
    "DATA_DIR = \"../data\"\n",
    "RAW_DIR = f\"{DATA_DIR}/raw\"\n",
    "PROCESSED_DIR = f\"{DATA_DIR}/processed\"\n",
    "\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Download KorMedMCQA (Primary Medical Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Downloading KorMedMCQA (doctor config only to save memory)...\")\n\n# Load just doctor config to save memory\ntry:\n    kormedmcqa_doctor = load_dataset(\"sean0042/KorMedMCQA\", \"doctor\")\n    print(f\"Loaded doctor config: {kormedmcqa_doctor}\")\n    \n    # Show sample\n    print(f\"\\nSample entry:\")\n    split = 'train' if 'train' in kormedmcqa_doctor else list(kormedmcqa_doctor.keys())[0]\n    sample = kormedmcqa_doctor[split][0]\n    for key, value in sample.items():\n        print(f\"  {key}: {value}\")\n    \n    kormedmcqa_all = {'doctor': kormedmcqa_doctor}\nexcept Exception as e:\n    print(f\"Error loading KorMedMCQA: {e}\")\n    kormedmcqa_all = {}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick statistics\nprint(\"Dataset Statistics:\")\nif kormedmcqa_all:\n    for config, ds in kormedmcqa_all.items():\n        for split in ds.keys():\n            print(f\"  {config}/{split}: {len(ds[split])} examples\")\n    \n    # Clean up to save memory\n    import gc\n    gc.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save KorMedMCQA (all configs combined)\nkormedmcqa_path = f\"{RAW_DIR}/kormedmcqa\"\n\n# Combine all configs into one DatasetDict\nfrom datasets import DatasetDict\n\ncombined_train = []\ncombined_test = []\n\nfor config, ds in kormedmcqa_all.items():\n    for split in ds.keys():\n        for item in ds[split]:\n            item_dict = dict(item)\n            item_dict['exam_type'] = config  # Add exam type as column\n            if 'train' in split.lower():\n                combined_train.append(item_dict)\n            else:\n                combined_test.append(item_dict)\n\n# Create combined dataset\ncombined_ds = DatasetDict({\n    'train': Dataset.from_list(combined_train) if combined_train else Dataset.from_list([]),\n    'test': Dataset.from_list(combined_test) if combined_test else Dataset.from_list([]),\n})\n\ncombined_ds.save_to_disk(kormedmcqa_path)\nprint(f\"Saved combined KorMedMCQA to {kormedmcqa_path}\")\nprint(f\"  Train: {len(combined_ds['train'])}, Test: {len(combined_ds['test'])}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Download Medical Reasoning Dataset (Chain-of-Thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Medical Reasoning KorMedMCQA:\")\nprint(\"  Source: ChuGyouk/medical-reasoning-train-kormedmcqa\")\nprint(\"  Skipping to save memory - can be loaded later\")\nmed_reasoning = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Download KorMedLawQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"KorMedLawQA:\")\nprint(\"  Source: snuh/KorMedLawQA\")\nprint(\"  Skipping to save memory - can be loaded later\")\nkormedlawqa = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Filter Medical Content from Korean Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medical keywords for filtering\n",
    "MEDICAL_KEYWORDS_KO = [\n",
    "    # Diseases\n",
    "    \"질병\", \"질환\", \"증후군\", \"암\", \"종양\", \"감염\", \"바이러스\", \"세균\",\n",
    "    \"당뇨\", \"고혈압\", \"뇌졸중\", \"심장\", \"폐렴\", \"간염\", \"신장\",\n",
    "    \n",
    "    # Medical practice\n",
    "    \"의학\", \"의료\", \"치료\", \"진단\", \"수술\", \"처방\", \"투약\", \"주사\",\n",
    "    \"병원\", \"의사\", \"간호사\", \"환자\", \"약사\", \"약물\", \"약품\",\n",
    "    \n",
    "    # Body parts\n",
    "    \"심장\", \"폐\", \"간\", \"신장\", \"위장\", \"뇌\", \"혈관\", \"뼈\", \"근육\",\n",
    "    \"피부\", \"눈\", \"귀\", \"코\", \"목\", \"장기\",\n",
    "    \n",
    "    # Symptoms\n",
    "    \"증상\", \"통증\", \"발열\", \"기침\", \"두통\", \"피로\", \"구토\", \"설사\",\n",
    "    \"염증\", \"부종\", \"출혈\",\n",
    "    \n",
    "    # Medical specialties\n",
    "    \"내과\", \"외과\", \"소아과\", \"산부인과\", \"정신과\", \"피부과\", \"안과\",\n",
    "    \"이비인후과\", \"치과\", \"응급의학\", \"마취과\",\n",
    "    \n",
    "    # Health\n",
    "    \"건강\", \"면역\", \"예방\", \"백신\", \"검진\", \"혈액\", \"호르몬\",\n",
    "]\n",
    "\n",
    "def is_medical_article(text, title=\"\"):\n",
    "    \"\"\"Check if article is medical-related\"\"\"\n",
    "    combined = (title + \" \" + text).lower()\n",
    "    \n",
    "    # Count keyword matches\n",
    "    matches = sum(1 for kw in MEDICAL_KEYWORDS_KO if kw in combined)\n",
    "    \n",
    "    # Require at least 2 keyword matches\n",
    "    return matches >= 2\n",
    "\n",
    "print(f\"Medical keywords: {len(MEDICAL_KEYWORDS_KO)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Skip Wikipedia filtering for now - use smaller sample\nprint(\"Medical Wikipedia filtering:\")\nprint(\"  Skipping full Wikipedia scan to save memory\")\nprint(\"  Will use smaller sample for demo\")\n\nmedical_articles = []\nprint(f\"\\nMedical articles placeholder: {len(medical_articles)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Placeholder for medical articles\nprint(\"Sample medical articles: (skipped)\")\nprint(\"  Will be collected in full training run\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Skip saving medical Wikipedia for now\nprint(\"Medical Wikipedia saving: skipped\")\nwiki_medical = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Download General Korean Corpus (for Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Skip OSCAR for now - requires authentication\nprint(\"OSCAR Korean corpus:\")\nprint(\"  Note: OSCAR-2301 is a GATED dataset requiring HuggingFace authentication\")\nprint(\"  To access:\")\nprint(\"    1) Accept terms at https://huggingface.co/datasets/oscar-corpus/OSCAR-2301\")\nprint(\"    2) Run: huggingface-cli login\")\nprint(\"\\n  Skipping OSCAR download for now.\")\nprint(\"  Alternative: Use Korean Wikipedia for tokenizer training corpus\")\n\n# Create placeholder\ntokenizer_corpus = []\ntotal_chars = 0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create small sample corpus for tokenizer testing\nprint(\"Creating small sample corpus for tokenizer...\")\n\nwiki_corpus_path = f\"{RAW_DIR}/korean_corpus_for_tokenizer.txt\"\n\n# Just collect a small sample (100MB) for testing\ntry:\n    wiki_ko_stream = load_dataset(\"wikimedia/wikipedia\", \"20231101.ko\", split=\"train\", streaming=True)\n    \n    tokenizer_corpus = []\n    total_chars = 0\n    target_chars = 100 * 1024 * 1024  # 100MB for demo\n    \n    for i, article in enumerate(wiki_ko_stream):\n        text = article.get(\"text\", \"\")\n        if len(text) < 100:\n            continue\n        \n        tokenizer_corpus.append(text)\n        total_chars += len(text)\n        \n        if total_chars >= target_chars:\n            break\n        \n        if i % 10000 == 0 and i > 0:\n            print(f\"  Processed {i} articles, {total_chars / 1e6:.1f}MB\")\n    \n    print(f\"\\nCollected {len(tokenizer_corpus)} articles, {total_chars / 1e6:.1f}MB\")\n    \n    # Save\n    with open(wiki_corpus_path, \"w\", encoding=\"utf-8\") as f:\n        for text in tokenizer_corpus:\n            text = re.sub(r'\\s+', ' ', text).strip()\n            if text:\n                f.write(text + \"\\n\")\n    \n    file_size = os.path.getsize(wiki_corpus_path) / (1024**3)\n    print(f\"Saved! File size: {file_size:.3f}GB\")\n\nexcept Exception as e:\n    print(f\"Error collecting corpus: {e}\")\n    tokenizer_corpus = []\n    file_size = 0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create summary of collected data\ncollected_datasets = {\n    \"medical\": {\n        \"kormedmcqa\": {\n            \"path\": f\"{RAW_DIR}/kormedmcqa\",\n            \"size\": len(combined_ds.get('train', [])) + len(combined_ds.get('test', [])),\n            \"type\": \"QA\",\n        },\n    },\n    \"general_corpus\": {},\n}\n\n# Add tokenizer corpus if we collected it\nif tokenizer_corpus:\n    collected_datasets[\"general_corpus\"][\"tokenizer_corpus\"] = {\n        \"path\": wiki_corpus_path,\n        \"size_gb\": file_size,\n        \"num_texts\": len(tokenizer_corpus),\n    }\n\n# Add optional datasets\nif 'med_reasoning' in dir() and med_reasoning:\n    collected_datasets[\"medical\"][\"medical_reasoning\"] = {\n        \"path\": f\"{RAW_DIR}/medical_reasoning_kormedmcqa\",\n        \"type\": \"QA + CoT\",\n    }\n\nif 'kormedlawqa' in dir() and kormedlawqa:\n    collected_datasets[\"medical\"][\"kormedlawqa\"] = {\n        \"path\": f\"{RAW_DIR}/kormedlawqa\",\n        \"type\": \"Medical Law QA\",\n    }\n\nif 'medical_articles' in dir() and medical_articles:\n    collected_datasets[\"medical\"][\"wiki_medical_ko\"] = {\n        \"path\": f\"{RAW_DIR}/wiki_medical_ko\",\n        \"size\": len(medical_articles),\n        \"type\": \"Medical Wikipedia\",\n    }\n\n# Save summary\nwith open(f\"{DATA_DIR}/collected_datasets.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(collected_datasets, f, ensure_ascii=False, indent=2)\n\nprint(\"=\" * 60)\nprint(\"Data Collection Summary\")\nprint(\"=\" * 60)\nprint(json.dumps(collected_datasets, indent=2, ensure_ascii=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Korean Medical Data Collection Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nData saved to: {RAW_DIR}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Run 03_collect_bilingual_dict.ipynb to create bilingual dictionary\")\n",
    "print(\"  2. Run 04_preprocess_data.ipynb to prepare training data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}