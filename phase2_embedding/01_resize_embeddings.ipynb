{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2.1: Resize Model Embeddings\n",
    "\n",
    "Resize MedGemma embedding layers to accommodate new Korean tokens.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Load Model\n",
    "2. Load Merged Tokenizer\n",
    "3. Resize Embeddings\n",
    "4. Verify Resized Model\n",
    "5. Save Resized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "\n",
    "# GPU setup\n",
    "from config.gpu_utils import setup_gpu, print_memory_usage\n",
    "device = setup_gpu()\n",
    "\n",
    "# Directories\n",
    "MERGED_TOKENIZER_DIR = \"../models/merged_tokenizer\"\n",
    "RESIZED_MODEL_DIR = \"../models/resized_model\"\n",
    "\n",
    "os.makedirs(RESIZED_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {RESIZED_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load token mapping to get base model info\n",
    "mapping_path = f\"{MERGED_TOKENIZER_DIR}/token_mapping.json\"\n",
    "\n",
    "with open(mapping_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    token_mapping = json.load(f)\n",
    "\n",
    "BASE_MODEL = token_mapping[\"base_model\"]\n",
    "original_vocab_size = token_mapping[\"original_vocab_size\"]\n",
    "new_vocab_size = token_mapping[\"new_vocab_size\"]\n",
    "\n",
    "print(f\"Base model: {BASE_MODEL}\")\n",
    "print(f\"Original vocab size: {original_vocab_size}\")\n",
    "print(f\"New vocab size: {new_vocab_size}\")\n",
    "print(f\"New tokens: {new_vocab_size - original_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(f\"\\nLoading base model: {BASE_MODEL}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",  # Load on CPU for embedding manipulation\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture info\n",
    "print(\"\\nModel architecture:\")\n",
    "print(f\"  Model type: {model.config.model_type}\")\n",
    "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"  Num layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"  Num heads: {model.config.num_attention_heads}\")\n",
    "print(f\"  Vocab size (config): {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding layer info\n",
    "input_embeddings = model.get_input_embeddings()\n",
    "output_embeddings = model.get_output_embeddings()  # lm_head\n",
    "\n",
    "print(\"\\nEmbedding layers:\")\n",
    "print(f\"  Input embeddings shape: {input_embeddings.weight.shape}\")\n",
    "print(f\"  Output embeddings shape: {output_embeddings.weight.shape}\")\n",
    "print(f\"  Embedding dim: {input_embeddings.weight.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Merged Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merged tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MERGED_TOKENIZER_DIR)\n",
    "\n",
    "print(f\"Loaded merged tokenizer\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# Verify sizes match\n",
    "assert len(tokenizer) == new_vocab_size, f\"Vocab size mismatch: {len(tokenizer)} vs {new_vocab_size}\"\n",
    "print(f\"\\nVocab size verified: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Resize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record original embedding weights (for later initialization)\n",
    "original_input_embeds = input_embeddings.weight.data.clone()\n",
    "original_output_embeds = output_embeddings.weight.data.clone()\n",
    "\n",
    "print(f\"Saved original embeddings\")\n",
    "print(f\"  Input shape: {original_input_embeds.shape}\")\n",
    "print(f\"  Output shape: {original_output_embeds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize token embeddings\n",
    "print(f\"\\nResizing embeddings: {original_vocab_size} -> {new_vocab_size}\")\n",
    "\n",
    "model.resize_token_embeddings(new_vocab_size)\n",
    "\n",
    "# Verify resize\n",
    "new_input_embeddings = model.get_input_embeddings()\n",
    "new_output_embeddings = model.get_output_embeddings()\n",
    "\n",
    "print(f\"\\nResized embeddings:\")\n",
    "print(f\"  Input embeddings shape: {new_input_embeddings.weight.shape}\")\n",
    "print(f\"  Output embeddings shape: {new_output_embeddings.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify original embeddings are preserved\n",
    "preserved_input = torch.allclose(\n",
    "    new_input_embeddings.weight.data[:original_vocab_size],\n",
    "    original_input_embeds,\n",
    "    atol=1e-6\n",
    ")\n",
    "preserved_output = torch.allclose(\n",
    "    new_output_embeddings.weight.data[:original_vocab_size],\n",
    "    original_output_embeds,\n",
    "    atol=1e-6\n",
    ")\n",
    "\n",
    "print(f\"\\nOriginal embeddings preserved:\")\n",
    "print(f\"  Input embeddings: {preserved_input}\")\n",
    "print(f\"  Output embeddings: {preserved_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check new token embeddings (should be randomly initialized or zero)\n",
    "new_token_input_embeds = new_input_embeddings.weight.data[original_vocab_size:]\n",
    "new_token_output_embeds = new_output_embeddings.weight.data[original_vocab_size:]\n",
    "\n",
    "print(f\"\\nNew token embeddings (before initialization):\")\n",
    "print(f\"  Input - Mean: {new_token_input_embeds.mean():.6f}, Std: {new_token_input_embeds.std():.6f}\")\n",
    "print(f\"  Output - Mean: {new_token_output_embeds.mean():.6f}, Std: {new_token_output_embeds.std():.6f}\")\n",
    "print(f\"\\nNote: These will be properly initialized in the next notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Update Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model config\n",
    "model.config.vocab_size = new_vocab_size\n",
    "\n",
    "print(f\"Updated model config:\")\n",
    "print(f\"  vocab_size: {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Verify Resized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass with original tokens\n",
    "print(\"Testing forward pass with original tokens...\")\n",
    "\n",
    "test_input = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "print(f\"Input IDs: {test_input['input_ids']}\")\n",
    "print(f\"Max ID: {test_input['input_ids'].max().item()}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_input)\n",
    "\n",
    "print(f\"\\nOutput logits shape: {outputs.logits.shape}\")\n",
    "print(f\"Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass with new Korean tokens\n",
    "print(\"\\nTesting forward pass with Korean tokens...\")\n",
    "\n",
    "korean_test = \"안녕하세요, 의료 질문이 있습니다.\"\n",
    "korean_input = tokenizer(korean_test, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Korean text: {korean_test}\")\n",
    "print(f\"Input IDs: {korean_input['input_ids']}\")\n",
    "print(f\"Max ID: {korean_input['input_ids'].max().item()}\")\n",
    "print(f\"Vocab size: {new_vocab_size}\")\n",
    "\n",
    "# Check if any IDs exceed vocab size\n",
    "max_id = korean_input['input_ids'].max().item()\n",
    "if max_id >= new_vocab_size:\n",
    "    print(f\"WARNING: Token ID {max_id} exceeds vocab size {new_vocab_size}!\")\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**korean_input)\n",
    "    print(f\"\\nOutput logits shape: {outputs.logits.shape}\")\n",
    "    print(f\"Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Save Resized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save resized model\n",
    "print(f\"\\nSaving resized model to {RESIZED_MODEL_DIR}...\")\n",
    "\n",
    "model.save_pretrained(RESIZED_MODEL_DIR)\n",
    "tokenizer.save_pretrained(RESIZED_MODEL_DIR)\n",
    "\n",
    "print(\"Model and tokenizer saved!\")\n",
    "\n",
    "# List saved files\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(RESIZED_MODEL_DIR):\n",
    "    size = os.path.getsize(os.path.join(RESIZED_MODEL_DIR, f)) / (1024**2)\n",
    "    print(f\"  {f}: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save resize info\n",
    "resize_info = {\n",
    "    \"base_model\": BASE_MODEL,\n",
    "    \"original_vocab_size\": original_vocab_size,\n",
    "    \"new_vocab_size\": new_vocab_size,\n",
    "    \"new_tokens_added\": new_vocab_size - original_vocab_size,\n",
    "    \"embedding_dim\": model.config.hidden_size,\n",
    "    \"embedding_initialization\": \"random (needs proper initialization)\",\n",
    "}\n",
    "\n",
    "info_path = f\"{RESIZED_MODEL_DIR}/resize_info.json\"\n",
    "with open(info_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(resize_info, f, indent=2)\n",
    "\n",
    "print(f\"\\nResize info saved to {info_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy token mapping for embedding initialization\n",
    "import shutil\n",
    "\n",
    "src_mapping = f\"{MERGED_TOKENIZER_DIR}/token_mapping.json\"\n",
    "dst_mapping = f\"{RESIZED_MODEL_DIR}/token_mapping.json\"\n",
    "shutil.copy(src_mapping, dst_mapping)\n",
    "\n",
    "print(f\"Copied token mapping to {dst_mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Model Embedding Resize Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nResized model saved to: {RESIZED_MODEL_DIR}\")\n",
    "print(f\"Vocabulary: {original_vocab_size} -> {new_vocab_size} (+{new_vocab_size - original_vocab_size})\")\n",
    "print(\"\\nIMPORTANT: New token embeddings are randomly initialized.\")\n",
    "print(\"Run 02_initialize_embeddings.ipynb to properly initialize them.\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Run 02_initialize_embeddings.ipynb for EEVE/WECHSEL initialization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
