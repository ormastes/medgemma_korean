{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2.2: Initialize New Token Embeddings\n",
    "\n",
    "Initialize new Korean token embeddings using hybrid method:\n",
    "- EEVE: Subword decomposition (primary)\n",
    "- WECHSEL: Bilingual dictionary alignment (for medical terms)\n",
    "\n",
    "## Contents\n",
    "1. Setup and Load Model\n",
    "2. Load Original Tokenizer (for subword decomposition)\n",
    "3. Load Bilingual Dictionary\n",
    "4. Implement Initialization Methods\n",
    "5. Initialize All New Embeddings\n",
    "6. Verify Initialization\n",
    "7. Save Initialized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU setup\n",
    "from config.gpu_utils import setup_gpu, print_memory_usage, clear_memory\n",
    "\n",
    "# Directories\n",
    "RESIZED_MODEL_DIR = \"../models/resized_model\"\n",
    "INITIALIZED_MODEL_DIR = \"../models/initialized_model\"\n",
    "BILINGUAL_DICT_DIR = \"../data/bilingual_dict\"\n",
    "\n",
    "os.makedirs(INITIALIZED_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {INITIALIZED_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Resized Model and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load token mapping\n",
    "mapping_path = f\"{RESIZED_MODEL_DIR}/token_mapping.json\"\n",
    "with open(mapping_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    token_mapping = json.load(f)\n",
    "\n",
    "BASE_MODEL = token_mapping[\"base_model\"]\n",
    "original_vocab_size = token_mapping[\"original_vocab_size\"]\n",
    "new_vocab_size = token_mapping[\"new_vocab_size\"]\n",
    "new_tokens = token_mapping[\"new_tokens\"]\n",
    "\n",
    "print(f\"Base model: {BASE_MODEL}\")\n",
    "print(f\"Original vocab: {original_vocab_size}\")\n",
    "print(f\"New vocab: {new_vocab_size}\")\n",
    "print(f\"New tokens: {len(new_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load resized model\n",
    "print(\"\\nLoading resized model...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    RESIZED_MODEL_DIR,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",  # CPU for embedding manipulation\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Vocab size: {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new tokenizer (with Korean tokens)\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(RESIZED_MODEL_DIR)\n",
    "print(f\"New tokenizer vocab size: {len(new_tokenizer)}\")\n",
    "\n",
    "# Load original tokenizer (for subword decomposition)\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "print(f\"Original tokenizer vocab size: {len(old_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Bilingual Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bilingual medical dictionary\n",
    "dict_path = f\"{BILINGUAL_DICT_DIR}/bilingual_medical_dict.json\"\n",
    "\n",
    "if os.path.exists(dict_path):\n",
    "    with open(dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        bilingual_dict = json.load(f)\n",
    "    print(f\"Loaded bilingual dictionary: {len(bilingual_dict)} entries\")\n",
    "    \n",
    "    # Create reverse mapping (Korean -> English)\n",
    "    ko_to_en = {v: k for k, v in bilingual_dict.items()}\n",
    "    print(f\"Korean to English mapping: {len(ko_to_en)} entries\")\n",
    "else:\n",
    "    print(f\"Bilingual dictionary not found at {dict_path}\")\n",
    "    print(\"Will use subword decomposition only.\")\n",
    "    bilingual_dict = {}\n",
    "    ko_to_en = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Get Embedding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding layers\n",
    "input_embeds = model.get_input_embeddings().weight.data\n",
    "output_embeds = model.get_output_embeddings().weight.data\n",
    "\n",
    "print(f\"Input embeddings shape: {input_embeds.shape}\")\n",
    "print(f\"Output embeddings shape: {output_embeds.shape}\")\n",
    "print(f\"Embedding dimension: {input_embeds.shape[1]}\")\n",
    "\n",
    "embedding_dim = input_embeds.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current state of new embeddings\n",
    "new_input_embeds = input_embeds[original_vocab_size:]\n",
    "new_output_embeds = output_embeds[original_vocab_size:]\n",
    "\n",
    "print(f\"\\nNew embeddings before initialization:\")\n",
    "print(f\"  Input - Mean: {new_input_embeds.mean():.6f}, Std: {new_input_embeds.std():.6f}\")\n",
    "print(f\"  Output - Mean: {new_output_embeds.mean():.6f}, Std: {new_output_embeds.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Implement Initialization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridEmbeddingInitializer:\n",
    "    \"\"\"\n",
    "    Hybrid embedding initialization combining:\n",
    "    - EEVE: Subword decomposition\n",
    "    - WECHSEL: Bilingual dictionary alignment\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_embeds,\n",
    "        output_embeds,\n",
    "        new_tokenizer,\n",
    "        old_tokenizer,\n",
    "        original_vocab_size,\n",
    "        bilingual_dict=None,\n",
    "    ):\n",
    "        self.input_embeds = input_embeds\n",
    "        self.output_embeds = output_embeds\n",
    "        self.new_tokenizer = new_tokenizer\n",
    "        self.old_tokenizer = old_tokenizer\n",
    "        self.original_vocab_size = original_vocab_size\n",
    "        \n",
    "        # Create Korean -> English mapping\n",
    "        self.ko_to_en = {}\n",
    "        if bilingual_dict:\n",
    "            self.ko_to_en = {v: k for k, v in bilingual_dict.items()}\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            \"subword_average\": 0,\n",
    "            \"bilingual_aligned\": 0,\n",
    "            \"mean_fallback\": 0,\n",
    "            \"total\": 0,\n",
    "        }\n",
    "    \n",
    "    def get_subword_ids(self, token):\n",
    "        \"\"\"Tokenize token with old tokenizer to get subword IDs\"\"\"\n",
    "        ids = self.old_tokenizer.encode(token, add_special_tokens=False)\n",
    "        # Filter out special token IDs if any\n",
    "        ids = [i for i in ids if i < self.original_vocab_size]\n",
    "        return ids\n",
    "    \n",
    "    def find_english_equivalent(self, korean_token):\n",
    "        \"\"\"Find English equivalent for Korean token\"\"\"\n",
    "        # Direct lookup\n",
    "        if korean_token in self.ko_to_en:\n",
    "            return self.ko_to_en[korean_token]\n",
    "        \n",
    "        # Check if token is part of a longer Korean word\n",
    "        for ko, en in self.ko_to_en.items():\n",
    "            if korean_token in ko or ko in korean_token:\n",
    "                return en\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def initialize_with_subword_average(self, token, token_idx):\n",
    "        \"\"\"\n",
    "        EEVE method: Initialize using subword decomposition.\n",
    "        - Input embeddings: Average of all subword embeddings\n",
    "        - Output embeddings: First subword only\n",
    "        \"\"\"\n",
    "        subword_ids = self.get_subword_ids(token)\n",
    "        \n",
    "        if len(subword_ids) == 0:\n",
    "            return False\n",
    "        \n",
    "        # Input: Average of all subwords\n",
    "        self.input_embeds[token_idx] = self.input_embeds[subword_ids].mean(dim=0)\n",
    "        \n",
    "        # Output: First subword only (EEVE finding)\n",
    "        self.output_embeds[token_idx] = self.output_embeds[subword_ids[0]]\n",
    "        \n",
    "        self.stats[\"subword_average\"] += 1\n",
    "        return True\n",
    "    \n",
    "    def initialize_with_bilingual_alignment(self, token, token_idx):\n",
    "        \"\"\"\n",
    "        WECHSEL method: Initialize using bilingual dictionary alignment.\n",
    "        \"\"\"\n",
    "        english_equiv = self.find_english_equivalent(token)\n",
    "        \n",
    "        if english_equiv is None:\n",
    "            return False\n",
    "        \n",
    "        # Get English token embeddings\n",
    "        en_ids = self.get_subword_ids(english_equiv)\n",
    "        \n",
    "        if len(en_ids) == 0:\n",
    "            return False\n",
    "        \n",
    "        # Use average of English subword embeddings\n",
    "        self.input_embeds[token_idx] = self.input_embeds[en_ids].mean(dim=0)\n",
    "        self.output_embeds[token_idx] = self.output_embeds[en_ids[0]]\n",
    "        \n",
    "        self.stats[\"bilingual_aligned\"] += 1\n",
    "        return True\n",
    "    \n",
    "    def initialize_with_mean_fallback(self, token_idx):\n",
    "        \"\"\"\n",
    "        Fallback: Use mean of all original embeddings.\n",
    "        \"\"\"\n",
    "        self.input_embeds[token_idx] = self.input_embeds[:self.original_vocab_size].mean(dim=0)\n",
    "        self.output_embeds[token_idx] = self.output_embeds[:self.original_vocab_size].mean(dim=0)\n",
    "        \n",
    "        self.stats[\"mean_fallback\"] += 1\n",
    "        return True\n",
    "    \n",
    "    def initialize_token(self, token, prefer_bilingual=True):\n",
    "        \"\"\"\n",
    "        Initialize a single token embedding using hybrid method.\n",
    "        \"\"\"\n",
    "        # Get token ID\n",
    "        token_idx = self.new_tokenizer.convert_tokens_to_ids(token)\n",
    "        \n",
    "        # Skip if it's an original token\n",
    "        if token_idx < self.original_vocab_size:\n",
    "            return \"original\"\n",
    "        \n",
    "        # Skip if it's UNK\n",
    "        if token_idx == self.new_tokenizer.unk_token_id:\n",
    "            return \"unk\"\n",
    "        \n",
    "        self.stats[\"total\"] += 1\n",
    "        \n",
    "        # Try bilingual alignment first (for medical terms)\n",
    "        if prefer_bilingual and self.ko_to_en:\n",
    "            if self.initialize_with_bilingual_alignment(token, token_idx):\n",
    "                return \"bilingual_aligned\"\n",
    "        \n",
    "        # Try subword decomposition\n",
    "        if self.initialize_with_subword_average(token, token_idx):\n",
    "            return \"subword_average\"\n",
    "        \n",
    "        # Fallback to mean\n",
    "        self.initialize_with_mean_fallback(token_idx)\n",
    "        return \"mean_fallback\"\n",
    "    \n",
    "    def initialize_all(self, tokens, prefer_bilingual=True):\n",
    "        \"\"\"\n",
    "        Initialize all new tokens.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for token in tqdm(tokens, desc=\"Initializing embeddings\"):\n",
    "            method = self.initialize_token(token, prefer_bilingual)\n",
    "            results.append((token, method))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return self.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Initialize All New Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initializer\n",
    "initializer = HybridEmbeddingInitializer(\n",
    "    input_embeds=input_embeds,\n",
    "    output_embeds=output_embeds,\n",
    "    new_tokenizer=new_tokenizer,\n",
    "    old_tokenizer=old_tokenizer,\n",
    "    original_vocab_size=original_vocab_size,\n",
    "    bilingual_dict=bilingual_dict if bilingual_dict else None,\n",
    ")\n",
    "\n",
    "print(f\"Initializer created\")\n",
    "print(f\"Bilingual dictionary entries: {len(initializer.ko_to_en)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all new tokens\n",
    "print(f\"\\nInitializing {len(new_tokens)} new token embeddings...\")\n",
    "\n",
    "results = initializer.initialize_all(new_tokens, prefer_bilingual=True)\n",
    "\n",
    "# Print statistics\n",
    "stats = initializer.get_stats()\n",
    "print(f\"\\nInitialization Statistics:\")\n",
    "for method, count in stats.items():\n",
    "    if count > 0:\n",
    "        pct = count / stats[\"total\"] * 100 if stats[\"total\"] > 0 else 0\n",
    "        print(f\"  {method}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample results\n",
    "print(\"\\nSample initialization results:\")\n",
    "print(f\"{'Token':<20} | {'Method':<20}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Show some bilingual aligned\n",
    "bilingual_samples = [r for r in results if r[1] == \"bilingual_aligned\"][:5]\n",
    "for token, method in bilingual_samples:\n",
    "    print(f\"{token:<20} | {method:<20}\")\n",
    "\n",
    "# Show some subword average\n",
    "subword_samples = [r for r in results if r[1] == \"subword_average\"][:5]\n",
    "for token, method in subword_samples:\n",
    "    print(f\"{token:<20} | {method:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Verify Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check embedding statistics after initialization\n",
    "new_input_embeds_after = input_embeds[original_vocab_size:]\n",
    "new_output_embeds_after = output_embeds[original_vocab_size:]\n",
    "orig_input_embeds = input_embeds[:original_vocab_size]\n",
    "\n",
    "print(\"Embedding statistics after initialization:\")\n",
    "print(f\"\\nOriginal embeddings:\")\n",
    "print(f\"  Input - Mean: {orig_input_embeds.mean():.6f}, Std: {orig_input_embeds.std():.6f}\")\n",
    "\n",
    "print(f\"\\nNew embeddings (after initialization):\")\n",
    "print(f\"  Input - Mean: {new_input_embeds_after.mean():.6f}, Std: {new_input_embeds_after.std():.6f}\")\n",
    "print(f\"  Output - Mean: {new_output_embeds_after.mean():.6f}, Std: {new_output_embeds_after.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify specific medical terms are well-initialized\n",
    "medical_test_terms = [\"의사\", \"환자\", \"병원\", \"치료\", \"진단\"]\n",
    "\n",
    "print(\"\\nMedical term embedding check:\")\n",
    "for term in medical_test_terms:\n",
    "    token_id = new_tokenizer.convert_tokens_to_ids(term)\n",
    "    \n",
    "    if token_id >= original_vocab_size:\n",
    "        embed = input_embeds[token_id]\n",
    "        norm = torch.norm(embed).item()\n",
    "        print(f\"  {term} (id={token_id}): norm={norm:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {term}: not in new tokens (may be subword)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model forward pass with initialized embeddings\n",
    "print(\"\\nTesting forward pass...\")\n",
    "\n",
    "test_text = \"환자가 발열 증상을 호소합니다.\"\n",
    "inputs = new_tokenizer(test_text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(f\"Input: {test_text}\")\n",
    "print(f\"Token IDs: {inputs['input_ids'].tolist()}\")\n",
    "print(f\"Output logits shape: {outputs.logits.shape}\")\n",
    "print(f\"Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Save Initialized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save initialized model\n",
    "print(f\"\\nSaving initialized model to {INITIALIZED_MODEL_DIR}...\")\n",
    "\n",
    "model.save_pretrained(INITIALIZED_MODEL_DIR)\n",
    "new_tokenizer.save_pretrained(INITIALIZED_MODEL_DIR)\n",
    "\n",
    "print(\"Model and tokenizer saved!\")\n",
    "\n",
    "# List saved files\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(INITIALIZED_MODEL_DIR):\n",
    "    size = os.path.getsize(os.path.join(INITIALIZED_MODEL_DIR, f)) / (1024**2)\n",
    "    print(f\"  {f}: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save initialization info\n",
    "init_info = {\n",
    "    \"base_model\": BASE_MODEL,\n",
    "    \"original_vocab_size\": original_vocab_size,\n",
    "    \"new_vocab_size\": new_vocab_size,\n",
    "    \"new_tokens_count\": len(new_tokens),\n",
    "    \"initialization_method\": \"hybrid (EEVE + WECHSEL)\",\n",
    "    \"statistics\": stats,\n",
    "    \"bilingual_dict_size\": len(bilingual_dict) if bilingual_dict else 0,\n",
    "}\n",
    "\n",
    "info_path = f\"{INITIALIZED_MODEL_DIR}/initialization_info.json\"\n",
    "with open(info_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(init_info, f, indent=2)\n",
    "\n",
    "print(f\"\\nInitialization info saved to {info_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy token mapping\n",
    "import shutil\n",
    "\n",
    "src_mapping = f\"{RESIZED_MODEL_DIR}/token_mapping.json\"\n",
    "dst_mapping = f\"{INITIALIZED_MODEL_DIR}/token_mapping.json\"\n",
    "shutil.copy(src_mapping, dst_mapping)\n",
    "\n",
    "print(f\"Copied token mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Embedding Initialization Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nInitialized model saved to: {INITIALIZED_MODEL_DIR}\")\n",
    "print(f\"\\nInitialization breakdown:\")\n",
    "for method, count in stats.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {method}: {count}\")\n",
    "print(\"\\nPhase 2 Complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Move to Phase 3: Staged Training\")\n",
    "print(\"  2. Run phase3_staged_training/01_stage1_new_input_embeds.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
